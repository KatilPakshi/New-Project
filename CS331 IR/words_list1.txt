online
edition
c
2009
cambridge
up
an
introduction
to
information
retrieval
draft
of
april
1
2009
online
edition
c
2009
cambridge
up
online
edition
c
2009
cambridge
up
an
introduction
to
information
retrieval
christopher
d
manning
prabhakar
raghavan
hinrich
schã¼tze
cambridge
university
press
cambridge
england
online
edition
c
2009
cambridge
up
draft
do
not
distribute
without
prior
permission
â©
2009
cambridge
university
press
by
christopher
d
manning
prabhakar
raghavan
&
hinrich
schã¼tze
printed
on
april
1
2009
website
http://www.informationretrieval.org/
comments
corrections
and
other
feedback
most
welcome
at
informationretrieval@yahoogroups.com
online
edition
c
2009
cambridge
up
draft
â©
april
1
2009
cambridge
university
press
feedback
welcome
v
brief
contents
1
boolean
retrieval
1
2
the
term
vocabulary
and
postings
lists
19
3
dictionaries
and
tolerant
retrieval
49
4
index
construction
67
5
index
compression
85
6
scoring
term
weighting
and
the
vector
space
model
109
7
computing
scores
in
a
complete
search
system
135
8
evaluation
in
information
retrieval
151
9
relevance
feedback
and
query
expansion
177
10
xml
retrieval
195
11
probabilistic
information
retrieval
219
12
language
models
for
information
retrieval
237
13
text
classification
and
naive
bayes
253
14
vector
space
classification
289
15
support
vector
machines
and
machine
learning
on
documents
319
16
flat
clustering
349
17
hierarchical
clustering
377
18
matrix
decompositions
and
latent
semantic
indexing
403
19
web
search
basics
421
20
web
crawling
and
indexes
443
21
link
analysis
461
online
edition
c
2009
cambridge
up
online
edition
c
2009
cambridge
up
draft
â©
april
1
2009
cambridge
university
press
feedback
welcome
vii
contents
list
of
tables
xv
list
of
figures
xix
table
of
notation
xxvii
preface
xxxi
1
boolean
retrieval
1
1.1
an
example
information
retrieval
problem
3
1.2
a
first
take
at
building
an
inverted
index
6
1.3
processing
boolean
queries
10
1.4
the
extended
boolean
model
versus
ranked
retrieval
14
1.5
references
and
further
reading
17
2
the
term
vocabulary
and
postings
lists
19
2.1
document
delineation
and
character
sequence
decoding
19
2.1.1
obtaining
the
character
sequence
in
a
document
19
2.1.2
choosing
a
document
unit
20
2.2
determining
the
vocabulary
of
terms
22
2.2.1
tokenization
22
2.2.2
dropping
common
terms
stop
words
27
2.2.3
normalization
equivalence
classing
of
terms
28
2.2.4
stemming
and
lemmatization
32
2.3
faster
postings
list
intersection
via
skip
pointers
36
2.4
positional
postings
and
phrase
queries
39
2.4.1
biword
indexes
39
2.4.2
positional
indexes
41
2.4.3
combination
schemes
43
2.5
references
and
further
reading
45
online
edition
c
2009
cambridge
up
viii
contents
3
dictionaries
and
tolerant
retrieval
49
3.1
search
structures
for
dictionaries
49
3.2
wildcard
queries
51
3.2.1
general
wildcard
queries
53
3.2.2
k-gram
indexes
for
wildcard
queries
54
3.3
spelling
correction
56
3.3.1
implementing
spelling
correction
57
3.3.2
forms
of
spelling
correction
57
3.3.3
edit
distance
58
3.3.4
k-gram
indexes
for
spelling
correction
60
3.3.5
context
sensitive
spelling
correction
62
3.4
phonetic
correction
63
3.5
references
and
further
reading
65
4
index
construction
67
4.1
hardware
basics
68
4.2
blocked
sort-based
indexing
69
4.3
single-pass
in-memory
indexing
73
4.4
distributed
indexing
74
4.5
dynamic
indexing
78
4.6
other
types
of
indexes
80
4.7
references
and
further
reading
83
5
index
compression
85
5.1
statistical
properties
of
terms
in
information
retrieval
86
5.1.1
heapsâ€™
law
estimating
the
number
of
terms
88
5.1.2
zipfâ€™s
law
modeling
the
distribution
of
terms
89
5.2
dictionary
compression
90
5.2.1
dictionary
as
a
string
91
5.2.2
blocked
storage
92
5.3
postings
file
compression
95
5.3.1
variable
byte
codes
96
5.3.2
î³
codes
98
5.4
references
and
further
reading
105
6
scoring
term
weighting
and
the
vector
space
model
109
6.1
parametric
and
zone
indexes
110
6.1.1
weighted
zone
scoring
112
6.1.2
learning
weights
113
6.1.3
the
optimal
weight
g
115
6.2
term
frequency
and
weighting
117
6.2.1
inverse
document
frequency
117
6.2.2
tf-idf
weighting
118
online
edition
c
2009
cambridge
up
contents
ix
6.3
the
vector
space
model
for
scoring
120
6.3.1
dot
products
120
6.3.2
queries
as
vectors
123
6.3.3
computing
vector
scores
124
6.4
variant
tf-idf
functions
126
6.4.1
sublinear
tf
scaling
126
6.4.2
maximum
tf
normalization
127
6.4.3
document
and
query
weighting
schemes
128
6.4.4
pivoted
normalized
document
length
129
6.5
references
and
further
reading
133
7
computing
scores
in
a
complete
search
system
135
7.1
efficient
scoring
and
ranking
135
7.1.1
inexact
top
k
document
retrieval
137
7.1.2
index
elimination
137
7.1.3
champion
lists
138
7.1.4
static
quality
scores
and
ordering
138
7.1.5
impact
ordering
140
7.1.6
cluster
pruning
141
7.2
components
of
an
information
retrieval
system
143
7.2.1
tiered
indexes
143
7.2.2
query-term
proximity
144
7.2.3
designing
parsing
and
scoring
functions
145
7.2.4
putting
it
all
together
146
7.3
vector
space
scoring
and
query
operator
interaction
147
7.4
references
and
further
reading
149
8
evaluation
in
information
retrieval
151
8.1
information
retrieval
system
evaluation
152
8.2
standard
test
collections
153
8.3
evaluation
of
unranked
retrieval
sets
154
8.4
evaluation
of
ranked
retrieval
results
158
8.5
assessing
relevance
164
8.5.1
critiques
and
justifications
of
the
concept
of
relevance
166
8.6
a
broader
perspective
system
quality
and
user
utility
168
8.6.1
system
issues
168
8.6.2
user
utility
169
8.6.3
refining
a
deployed
system
170
8.7
results
snippets
170
8.8
references
and
further
reading
173
9
relevance
feedback
and
query
expansion
177
online
edition
c
2009
cambridge
up
x
contents
9.1
relevance
feedback
and
pseudo
relevance
feedback
178
9.1.1
the
rocchio
algorithm
for
relevance
feedback
178
9.1.2
probabilistic
relevance
feedback
183
9.1.3
when
does
relevance
feedback
work?
183
9.1.4
relevance
feedback
on
the
web
185
9.1.5
evaluation
of
relevance
feedback
strategies
186
9.1.6
pseudo
relevance
feedback
187
9.1.7
indirect
relevance
feedback
187
9.1.8
summary
188
9.2
global
methods
for
query
reformulation
189
9.2.1
vocabulary
tools
for
query
reformulation
189
9.2.2
query
expansion
189
9.2.3
automatic
thesaurus
generation
192
9.3
references
and
further
reading
193
10
xml
retrieval
195
10.1
basic
xml
concepts
197
10.2
challenges
in
xml
retrieval
201
10.3
a
vector
space
model
for
xml
retrieval
206
10.4
evaluation
of
xml
retrieval
210
10.5
text-centric
vs
data-centric
xml
retrieval
214
10.6
references
and
further
reading
216
10.7
exercises
217
11
probabilistic
information
retrieval
219
11.1
review
of
basic
probability
theory
220
11.2
the
probability
ranking
principle
221
11.2.1
the
1/0
loss
case
221
11.2.2
the
prp
with
retrieval
costs
222
11.3
the
binary
independence
model
222
11.3.1
deriving
a
ranking
function
for
query
terms
224
11.3.2
probability
estimates
in
theory
226
11.3.3
probability
estimates
in
practice
227
11.3.4
probabilistic
approaches
to
relevance
feedback
228
11.4
an
appraisal
and
some
extensions
230
11.4.1
an
appraisal
of
probabilistic
models
230
11.4.2
tree-structured
dependencies
between
terms
231
11.4.3
okapi
bm25
a
non-binary
model
232
11.4.4
bayesian
network
approaches
to
ir
234
11.5
references
and
further
reading
235
12
language
models
for
information
retrieval
237
12.1
language
models
237
online
edition
c
2009
cambridge
up
contents
xi
12.1.1
finite
automata
and
language
models
237
12.1.2
types
of
language
models
240
12.1.3
multinomial
distributions
over
words
241
12.2
the
query
likelihood
model
242
12.2.1
using
query
likelihood
language
models
in
ir
242
12.2.2
estimating
the
query
generation
probability
243
12.2.3
ponte
and
croftâ€™s
experiments
246
12.3
language
modeling
versus
other
approaches
in
ir
248
12.4
extended
language
modeling
approaches
250
12.5
references
and
further
reading
252
13
text
classification
and
naive
bayes
253
13.1
the
text
classification
problem
256
13.2
naive
bayes
text
classification
258
13.2.1
relation
to
multinomial
unigram
language
model
262
13.3
the
bernoulli
model
263
13.4
properties
of
naive
bayes
265
13.4.1
a
variant
of
the
multinomial
model
270
13.5
feature
selection
271
13.5.1
mutual
information
272
13.5.2
ï‡
2
feature
selection
275
13.5.3
frequency-based
feature
selection
277
13.5.4
feature
selection
for
multiple
classifiers
278
13.5.5
comparison
of
feature
selection
methods
278
13.6
evaluation
of
text
classification
279
13.7
references
and
further
reading
286
14
vector
space
classification
289
14.1
document
representations
and
measures
of
relatedness
in
vector
spaces
291
14.2
rocchio
classification
292
14.3
k
nearest
neighbor
297
14.3.1
time
complexity
and
optimality
of
knn
299
14.4
linear
versus
nonlinear
classifiers
301
14.5
classification
with
more
than
two
classes
306
14.6
the
bias-variance
tradeoff
308
14.7
references
and
further
reading
314
14.8
exercises
315
15
support
vector
machines
and
machine
learning
on
documents
319
15.1
support
vector
machines
the
linearly
separable
case
320
15.2
extensions
to
the
svm
model
327
15.2.1
soft
margin
classification
327
online
edition
c
2009
cambridge
up
xii
contents
15.2.2
multiclass
svms
330
15.2.3
nonlinear
svms
330
15.2.4
experimental
results
333
15.3
issues
in
the
classification
of
text
documents
334
15.3.1
choosing
what
kind
of
classifier
to
use
335
15.3.2
improving
classifier
performance
337
15.4
machine
learning
methods
in
ad
hoc
information
retrieval
341
15.4.1
a
simple
example
of
machine-learned
scoring
341
15.4.2
result
ranking
by
machine
learning
344
15.5
references
and
further
reading
346
16
flat
clustering
349
16.1
clustering
in
information
retrieval
350
16.2
problem
statement
354
16.2.1
cardinality
â€“
the
number
of
clusters
355
16.3
evaluation
of
clustering
356
16.4
k-means
360
16.4.1
cluster
cardinality
in
k-means
365
16.5
model-based
clustering
368
16.6
references
and
further
reading
372
16.7
exercises
374
17
hierarchical
clustering
377
17.1
hierarchical
agglomerative
clustering
378
17.2
single-link
and
complete-link
clustering
382
17.2.1
time
complexity
of
hac
385
17.3
group-average
agglomerative
clustering
388
17.4
centroid
clustering
391
17.5
optimality
of
hac
393
17.6
divisive
clustering
395
17.7
cluster
labeling
396
17.8
implementation
notes
398
17.9
references
and
further
reading
399
17.10
exercises
401
18
matrix
decompositions
and
latent
semantic
indexing
403
18.1
linear
algebra
review
403
18.1.1
matrix
decompositions
406
18.2
term-document
matrices
and
singular
value
decompositions
407
18.3
low-rank
approximations
410
18.4
latent
semantic
indexing
412
18.5
references
and
further
reading
417
online
edition
c
2009
cambridge
up
contents
xiii
19
web
search
basics
421
19.1
background
and
history
421
19.2
web
characteristics
423
19.2.1
the
web
graph
425
19.2.2
spam
427
19.3
advertising
as
the
economic
model
429
19.4
the
search
user
experience
432
19.4.1
user
query
needs
432
19.5
index
size
and
estimation
433
19.6
near-duplicates
and
shingling
437
19.7
references
and
further
reading
441
20
web
crawling
and
indexes
443
20.1
overview
443
20.1.1
features
a
crawler
must
provide
443
20.1.2
features
a
crawler
should
provide
444
20.2
crawling
444
20.2.1
crawler
architecture
445
20.2.2
dns
resolution
449
20.2.3
the
url
frontier
451
20.3
distributing
indexes
454
20.4
connectivity
servers
455
20.5
references
and
further
reading
458
21
link
analysis
461
21.1
the
web
as
a
graph
462
21.1.1
anchor
text
and
the
web
graph
462
21.2
pagerank
464
21.2.1
markov
chains
465
21.2.2
the
pagerank
computation
468
21.2.3
topic-specific
pagerank
471
21.3
hubs
and
authorities
474
21.3.1
choosing
the
subset
of
the
web
477
21.4
references
and
further
reading
480
bibliography
483
author
index
519
online
edition
c
2009
cambridge
up
online
edition
c
2009
cambridge
up
draft
â©
april
1
2009
cambridge
university
press
feedback
welcome
xv
list
of
tables
4.1
typical
system
parameters
in
2007
the
seek
time
is
the
time
needed
to
position
the
disk
head
in
a
new
position
the
transfer
time
per
byte
is
the
rate
of
transfer
from
disk
to
memory
when
the
head
is
in
the
right
position
68
4.2
collection
statistics
for
reuters-rcv1
values
are
rounded
for
the
computations
in
this
book
the
unrounded
values
are
806,791
documents
222
tokens
per
document
391,523
distinct
terms
6.04
bytes
per
token
with
spaces
and
punctuation
4.5
bytes
per
token
without
spaces
and
punctuation
7.5
bytes
per
term
and
96,969,056
tokens
the
numbers
in
this
table
correspond
to
the
third
line
â€œcase
foldingâ€
in
table
5.1
page
87
70
4.3
the
five
steps
in
constructing
an
index
for
reuters-rcv1
in
blocked
sort-based
indexing
line
numbers
refer
to
figure
4.2
82
4.4
collection
statistics
for
a
large
collection
82
5.1
the
effect
of
preprocessing
on
the
number
of
terms
nonpositional
postings
and
tokens
for
reuters-rcv1
â€œâˆ†%â€
indicates
the
reduction
in
size
from
the
previous
line
except
that
â€œ30
stop
wordsâ€
and
â€œ150
stop
wordsâ€
both
use
â€œcase
foldingâ€
as
their
reference
line
â€œt%â€
is
the
cumulative
â€œtotalâ€
reduction
from
unfiltered
we
performed
stemming
with
the
porter
stemmer
chapter
2
page
33
87
5.2
dictionary
compression
for
reuters-rcv1
95
5.3
encoding
gaps
instead
of
document
ids
for
example
we
store
gaps
107
5
43




instead
of
docids
283154
283159
283202



for
computer
the
first
docid
is
left
unchanged
only
shown
for
arachnocentric
96
5.4
vb
encoding
97
online
edition
c
2009
cambridge
up
xvi
list
of
tables
5.5
some
examples
of
unary
and
î³
codes
unary
codes
are
only
shown
for
the
smaller
numbers
commas
in
î³
codes
are
for
readability
only
and
are
not
part
of
the
actual
codes
98
5.6
index
and
dictionary
compression
for
reuters-rcv1
the
compression
ratio
depends
on
the
proportion
of
actual
text
in
the
collection
reuters-rcv1
contains
a
large
amount
of
xml
markup
using
the
two
best
compression
schemes
î³
encoding
and
blocking
with
front
coding
the
ratio
compressed
index
to
collection
size
is
therefore
especially
small
for
reuters-rcv1
101
+
5.9)/3600
â‰ˆ
0.03
103
5.7
two
gap
sequences
to
be
merged
in
blocked
sort-based
indexing
105
6.1
cosine
computation
for
exercise
6.19
132
8.1
calculation
of
11-point
interpolated
average
precision
159
8.2
calculating
the
kappa
statistic
165
10.1
rdb
relational
database
search
unstructured
information
retrieval
and
structured
information
retrieval
196
10.2
inex
2002
collection
statistics
211
10.3
inex
2002
results
of
the
vector
space
model
in
section
10.3
for
content-and-structure
cas
queries
and
the
quantization
function
q
213
10.4
a
comparison
of
content-only
and
full-structure
search
in
inex
2003/2004
214
13.1
data
for
parameter
estimation
examples
261
13.2
training
and
test
times
for
nb
261
13.3
multinomial
versus
bernoulli
model
268
13.4
correct
estimation
implies
accurate
prediction
but
accurate
prediction
does
not
imply
correct
estimation
269
13.5
a
set
of
documents
for
which
the
nb
independence
assumptions
are
problematic
270
13.6
critical
values
of
the
ï‡
2
distribution
with
one
degree
of
freedom
for
example
if
the
two
events
are
independent
then
p(x
2
>
6.63
<
0.01
so
for
x
2
>
6.63
the
assumption
of
independence
can
be
rejected
with
99%
confidence
277
13.7
the
ten
largest
classes
in
the
reuters-21578
collection
with
number
of
documents
in
training
and
test
sets
280
online
edition
c
2009
cambridge
up
list
of
tables
xvii
13.8
macro
and
microaveraging
â€œtruthâ€
is
the
true
class
and
â€œcallâ€
the
decision
of
the
classifier
in
this
example
macroaveraged
precision
is
10/(10
+
10
+
90/(10
+
90)]/2
=
0.5
+
0.9)/2
=
0.7
microaveraged
precision
is
100/(100
+
20
â‰ˆ
0.83
282
13.9
text
classification
effectiveness
numbers
on
reuters-21578
for
f1
in
percent
results
from
li
and
yang
2003
a
joachims
1998
b
knn
and
dumais
et
al
1998
b
nb
rocchio
trees
svm
282
13.10
data
for
parameter
estimation
exercise
284
14.1
vectors
and
class
centroids
for
the
data
in
table
13.1
294
14.2
training
and
test
times
for
rocchio
classification
296
14.3
training
and
test
times
for
knn
classification
299
14.4
a
linear
classifier
303
14.5
a
confusion
matrix
for
reuters-21578
308
15.1
training
and
testing
complexity
of
various
classifiers
including
svms
329
15.2
svm
classifier
break-even
f1
from
joachims
2002a
p
114
334
15.3
training
examples
for
machine-learned
scoring
342
16.1
some
applications
of
clustering
in
information
retrieval
351
16.2
the
four
external
evaluation
measures
applied
to
the
clustering
in
figure
16.4
357
16.3
the
em
clustering
algorithm
371
17.1
comparison
of
hac
algorithms
395
17.2
automatically
computed
cluster
labels
397
online
edition
c
2009
cambridge
up
online
edition
c
2009
cambridge
up
draft
â©
april
1
2009
cambridge
university
press
feedback
welcome
xix
list
of
figures
1.1
a
term-document
incidence
matrix
4
1.2
results
from
shakespeare
for
the
query
brutus
and
caesar
and
not
calpurnia
5
1.3
the
two
parts
of
an
inverted
index
7
1.4
building
an
index
by
sorting
and
grouping
8
1.5
intersecting
the
postings
lists
for
brutus
and
calpurnia
from
figure
1.3
10
1.6
algorithm
for
the
intersection
of
two
postings
lists
p1
and
p2
11
1.7
algorithm
for
conjunctive
queries
that
returns
the
set
of
documents
containing
each
term
in
the
input
list
of
terms
12
2.1
an
example
of
a
vocalized
modern
standard
arabic
word
21
2.2
the
conceptual
linear
order
of
characters
is
not
necessarily
the
order
that
you
see
on
the
page
21
2.3
the
standard
unsegmented
form
of
chinese
text
using
the
simplified
characters
of
mainland
china
26
2.4
ambiguities
in
chinese
word
segmentation
26
2.5
a
stop
list
of
25
semantically
non-selective
words
which
are
common
in
reuters-rcv1
26
2.6
an
example
of
how
asymmetric
expansion
of
query
terms
can
usefully
model
usersâ€™
expectations
28
2.7
japanese
makes
use
of
multiple
intermingled
writing
systems
and
like
chinese
does
not
segment
words
31
2.8
a
comparison
of
three
stemming
algorithms
on
a
sample
text
34
2.9
postings
lists
with
skip
pointers
36
2.10
postings
lists
intersection
with
skip
pointers
37
2.11
positional
index
example
41
2.12
an
algorithm
for
proximity
intersection
of
postings
lists
p1
and
p2
42
online
edition
c
2009
cambridge
up
xx
list
of
figures
3.1
a
binary
search
tree
51
3.2
a
b-tree
52
3.3
a
portion
of
a
permuterm
index
54
3.4
example
of
a
postings
list
in
a
3-gram
index
55
3.5
dynamic
programming
algorithm
for
computing
the
edit
distance
between
strings
s1
and
s2
59
3.6
example
levenshtein
distance
computation
59
3.7
matching
at
least
two
of
the
three
2-grams
in
the
query
bord
61
4.1
document
from
the
reuters
newswire
70
4.2
blocked
sort-based
indexing
71
4.3
merging
in
blocked
sort-based
indexing
72
4.4
inversion
of
a
block
in
single-pass
in-memory
indexing
73
4.5
an
example
of
distributed
indexing
with
mapreduce
adapted
from
dean
and
ghemawat
2004
76
4.6
map
and
reduce
functions
in
mapreduce
77
4.7
logarithmic
merging
each
token
termid,docid
is
initially
added
to
in-memory
index
z0
by
lmergeaddtoken
logarithmicmerge
initializes
z0
and
indexes
79
4.8
a
user-document
matrix
for
access
control
lists
element
i
j
is
1
if
user
i
has
access
to
document
j
and
0
otherwise
during
query
processing
a
userâ€™s
access
postings
list
is
intersected
with
the
results
list
returned
by
the
text
part
of
the
index
81
5.1
heapsâ€™
law
88
5.2
zipfâ€™s
law
for
reuters-rcv1
90
5.3
storing
the
dictionary
as
an
array
of
fixed-width
entries
91
5.4
dictionary-as-a-string
storage
92
5.5
blocked
storage
with
four
terms
per
block
93
5.6
search
of
the
uncompressed
dictionary
a
and
a
dictionary
compressed
by
blocking
with
k
=
4
b
94
5.7
front
coding
94
5.8
vb
encoding
and
decoding
97
5.9
entropy
h(p
as
a
function
of
p(x1
for
a
sample
space
with
two
outcomes
x1
and
x2
100
5.10
stratification
of
terms
for
estimating
the
size
of
a
î³
encoded
inverted
index
102
6.1
parametric
search
111
6.2
basic
zone
index
111
6.3
zone
index
in
which
the
zone
is
encoded
in
the
postings
rather
than
the
dictionary
111
online
edition
c
2009
cambridge
up
list
of
figures
xxi
6.4
algorithm
for
computing
the
weighted
zone
score
from
two
postings
lists
113
6.5
an
illustration
of
training
examples
115
6.6
the
four
possible
combinations
of
st
and
sb
115
6.7
collection
frequency
cf
and
document
frequency
df
behave
differently
as
in
this
example
from
the
reuters
collection
118
6.8
example
of
idf
values
119
6.9
table
of
tf
values
for
exercise
6.10
120
6.10
cosine
similarity
illustrated
121
6.11
euclidean
normalized
tf
values
for
documents
in
figure
6.9
122
6.12
term
frequencies
in
three
novels
122
6.13
term
vectors
for
the
three
novels
of
figure
6.12
123
6.14
the
basic
algorithm
for
computing
vector
space
scores
125
6.15
smart
notation
for
tf-idf
variants
128
6.16
pivoted
document
length
normalization
130
6.17
implementing
pivoted
document
length
normalization
by
linear
scaling
131
7.1
a
faster
algorithm
for
vector
space
scores
136
7.2
a
static
quality-ordered
index
139
7.3
cluster
pruning
142
7.4
tiered
indexes
144
7.5
a
complete
search
system
147
8.1
graph
comparing
the
harmonic
mean
to
other
means
157
8.2
precision/recall
graph
158
8.3
averaged
11-point
precision/recall
graph
across
50
queries
for
a
representative
trec
system
160
8.4
the
roc
curve
corresponding
to
the
precision-recall
curve
in
figure
8.2
162
8.5
an
example
of
selecting
text
for
a
dynamic
snippet
172
9.1
relevance
feedback
searching
over
images
179
9.2
example
of
relevance
feedback
on
a
text
collection
180
9.3
the
rocchio
optimal
query
for
separating
relevant
and
nonrelevant
documents
181
9.4
an
application
of
rocchioâ€™s
algorithm
182
9.5
results
showing
pseudo
relevance
feedback
greatly
improving
performance
187
9.6
an
example
of
query
expansion
in
the
interface
of
the
yahoo
web
search
engine
in
2006
190
9.7
examples
of
query
expansion
via
the
pubmed
thesaurus
191
9.8
an
example
of
an
automatically
generated
thesaurus
192
online
edition
c
2009
cambridge
up
xxii
list
of
figures
10.1
an
xml
document
198
10.2
the
xml
document
in
figure
10.1
as
a
simplified
dom
object
198
10.3
an
xml
query
in
nexi
format
and
its
partial
representation
as
a
tree
199
10.4
tree
representation
of
xml
documents
and
queries
200
10.5
partitioning
an
xml
document
into
non-overlapping
indexing
units
202
10.6
schema
heterogeneity
intervening
nodes
and
mismatched
names
204
10.7
a
structural
mismatch
between
two
queries
and
a
document
206
10.8
a
mapping
of
an
xml
document
left
to
a
set
of
lexicalized
subtrees
right
207
10.9
the
algorithm
for
scoring
documents
with
simnomerge
209
10.10
scoring
of
a
query
with
one
structural
term
in
simnomerge
209
10.11
simplified
schema
of
the
documents
in
the
inex
collection
211
11.1
a
tree
of
dependencies
between
terms
232
12.1
a
simple
finite
automaton
and
some
of
the
strings
in
the
language
it
generates
238
12.2
a
one-state
finite
automaton
that
acts
as
a
unigram
language
model
238
12.3
partial
specification
of
two
unigram
language
models
239
12.4
results
of
a
comparison
of
tf-idf
with
language
modeling
lm
term
weighting
by
ponte
and
croft
1998
247
12.5
three
ways
of
developing
the
language
modeling
approach
a
query
likelihood
b
document
likelihood
and
c
model
comparison
250
13.1
classes
training
set
and
test
set
in
text
classification

257
13.2
naive
bayes
algorithm
multinomial
model
training
and
testing
260
13.3
nb
algorithm
bernoulli
model
training
and
testing
263
13.4
the
multinomial
nb
model
266
13.5
the
bernoulli
nb
model
267
13.6
basic
feature
selection
algorithm
for
selecting
the
k
best
features
271
13.7
features
with
high
mutual
information
scores
for
six
reuters-rcv1
classes
274
13.8
effect
of
feature
set
size
on
accuracy
for
multinomial
and
bernoulli
models
275
13.9
a
sample
document
from
the
reuters-21578
collection
281
14.1
vector
space
classification
into
three
classes
290
online
edition
c
2009
cambridge
up
list
of
figures
xxiii
14.2
projections
of
small
areas
of
the
unit
sphere
preserve
distances
291
14.3
rocchio
classification
293
14.4
rocchio
classification
training
and
testing
295
14.5
the
multimodal
class
â€œaâ€
consists
of
two
different
clusters
small
upper
circles
centered
on
xâ€™s
295
14.6
voronoi
tessellation
and
decision
boundaries
double
lines
in
1nn
classification
297
14.7
knn
training
with
preprocessing
and
testing
298
14.8
there
are
an
infinite
number
of
hyperplanes
that
separate
two
linearly
separable
classes
301
14.9
linear
classification
algorithm
302
14.10
a
linear
problem
with
noise
304
14.11
a
nonlinear
problem
305
14.12
j
hyperplanes
do
not
divide
space
into
j
disjoint
regions
307
14.13
arithmetic
transformations
for
the
bias-variance
decomposition
310
14.14
example
for
differences
between
euclidean
distance
dot
product
similarity
and
cosine
similarity
316
14.15
a
simple
non-separable
set
of
points
317
15.1
the
support
vectors
are
the
5
points
right
up
against
the
margin
of
the
classifier
320
15.2
an
intuition
for
large-margin
classification
321
15.3
the
geometric
margin
of
a
point
r
and
a
decision
boundary
ï
323
15.4
a
tiny
3
data
point
training
set
for
an
svm
325
15.5
large
margin
classification
with
slack
variables
327
15.6
projecting
data
that
is
not
linearly
separable
into
a
higher
dimensional
space
can
make
it
linearly
separable
331
15.7
a
collection
of
training
examples
343
16.1
an
example
of
a
data
set
with
a
clear
cluster
structure
349
16.2
clustering
of
search
results
to
improve
recall
352
16.3
an
example
of
a
user
session
in
scatter-gather
353
16.4
purity
as
an
external
evaluation
criterion
for
cluster
quality
357
16.5
the
k-means
algorithm
361
16.6
a
k-means
example
for
k
=
2
in
r2

362
16.7
the
outcome
of
clustering
in
k-means
depends
on
the
initial
seeds
364
16.8
estimated
minimal
residual
sum
of
squares
as
a
function
of
the
number
of
clusters
in
k-means
366
17.1
a
dendrogram
of
a
single-link
clustering
of
30
documents
from
reuters-rcv1
379
17.2
a
simple
but
inefficient
hac
algorithm
381
online
edition
c
2009
cambridge
up
xxiv
list
of
figures
17.3
the
different
notions
of
cluster
similarity
used
by
the
four
hac
algorithms
381
17.4
a
single-link
left
and
complete-link
right
clustering
of
eight
documents
382
17.5
a
dendrogram
of
a
complete-link
clustering
383
17.6
chaining
in
single-link
clustering
384
17.7
outliers
in
complete-link
clustering
385
17.8
the
priority-queue
algorithm
for
hac
386
17.9
single-link
clustering
algorithm
using
an
nbm
array
387
17.10
complete-link
clustering
is
not
best-merge
persistent
388
17.11
three
iterations
of
centroid
clustering
391
17.12
centroid
clustering
is
not
monotonic
392
18.1
illustration
of
the
singular-value
decomposition
409
18.2
illustration
of
low
rank
approximation
using
the
singular-value
decomposition
411
18.3
the
documents
of
example
18.4
reduced
to
two
dimensions
in
v
â€²

t

416
18.4
documents
for
exercise
18.11
418
18.5
glossary
for
exercise
18.11
418
19.1
a
dynamically
generated
web
page
425
19.2
two
nodes
of
the
web
graph
joined
by
a
link
425
19.3
a
sample
small
web
graph
426
19.4
the
bowtie
structure
of
the
web
427
19.5
cloaking
as
used
by
spammers
428
19.6
search
advertising
triggered
by
query
keywords
431
19.7
the
various
components
of
a
web
search
engine
434
19.8
illustration
of
shingle
sketches
439
19.9
two
sets
sj1
and
sj2

their
jaccard
coefficient
is
2/5
440
20.1
the
basic
crawler
architecture
446
20.2
distributing
the
basic
crawl
architecture
449
20.3
the
url
frontier
452
20.4
example
of
an
auxiliary
hosts-to-back
queues
table
453
20.5
a
lexicographically
ordered
set
of
urls
456
20.6
a
four-row
segment
of
the
table
of
links
457
21.1
the
random
surfer
at
node
a
proceeds
with
probability
1/3
to
each
of
b
c
and
d
464
21.2
a
simple
markov
chain
with
three
states
the
numbers
on
the
links
indicate
the
transition
probabilities
466
21.3
the
sequence
of
probability
vectors
469
online
edition
c
2009
cambridge
up
list
of
figures
xxv
21.4
a
small
web
graph
470
21.5
topic-specific
pagerank
472
21.6
a
sample
run
of
hits
on
the
query
japan
elementary
schools
479
21.7
web
graph
for
exercise
21.22
480
online
edition
c
2009
cambridge
up
online
edition
c
2009
cambridge
up
draft
â©
april
1
2009
cambridge
university
press
feedback
welcome
xxvii
table
of
notation
symbol
page
meaning
î³
p
98
î³
code
î³
p
256
classification
or
clustering
function
î³(d
is
dâ€™s
class
or
cluster
î“
p
256
supervised
learning
method
in
chapters
13
and
14
î“(d
is
the
classification
function
î³
learned
from
training
set
d
î»
p
404
eigenvalue
~âµ
p
292
centroid
of
a
class
in
rocchio
classification
or
a
cluster
in
k-means
and
centroid
clustering
î¦
p
114
training
example
ïƒ
p
408
singular
value
î˜(â·
p
11
a
tight
bound
on
the
complexity
of
an
algorithm
ï‰
ï‰k
p
357
cluster
in
clustering
â„¦
p
357
clustering
or
set
of
clusters
{ï‰1




ï‰k}
arg
maxx
f(x
p
181
the
value
of
x
for
which
f
reaches
its
maximum
arg
minx
f(x
p
181
the
value
of
x
for
which
f
reaches
its
minimum
c
cj
p
256
class
or
category
in
classification
cft
p
89
the
collection
frequency
of
term
t
the
total
number
of
times
the
term
appears
in
the
document
collection
c
p
256
set
{c1





cj}
of
all
classes
c
p
268
a
random
variable
that
takes
as
values
members
of
c
online
edition
c
2009
cambridge
up
xxviii
table
of
notation
c
p
403
term-document
matrix
d
p
4
index
of
the
d
th
document
in
the
collection
d
d
p
71
a
document
d~,~q
p
181
document
vector
query
vector
d
p
354
set
{d1




dn}
of
all
documents
dc
p
292
set
of
documents
that
is
in
class
c
d
p
256
set
{hd1

c1i



hdn
cni}
of
all
labeled
documents
in
chapters
13â€“15
dft
p
118
the
document
frequency
of
term
t
the
total
number
of
documents
in
the
collection
the
term
appears
in
h
p
99
entropy
hm
p
101
mth
harmonic
number
i(x;y
p
272
mutual
information
of
random
variables
x
and
y
idft
p
118
inverse
document
frequency
of
term
t
j
p
256
number
of
classes
k
p
290
top
k
items
from
a
set
e.g
k
nearest
neighbors
in
knn
top
k
retrieved
documents
top
k
selected
features
from
the
vocabulary
v
k
p
54
sequence
of
k
characters
k
p
354
number
of
clusters
ld
p
233
length
of
document
d
in
tokens
la
p
262
length
of
the
test
document
or
application
document
in
tokens
lave
p
70
average
length
of
a
document
in
tokens
m
p
5
size
of
the
vocabulary
|v|
ma
p
262
size
of
the
vocabulary
of
the
test
document
or
application
document
mave
p
78
average
size
of
the
vocabulary
in
a
document
in
the
collection
md
p
237
language
model
for
document
d
n
p
4
number
of
documents
in
the
retrieval
or
training
collection
nc
p
259
number
of
documents
in
class
c
n(ï‰
p
298
number
of
times
the
event
ï‰
occurred
online
edition
c
2009
cambridge
up
table
of
notation
xxix
o(â·
p
11
a
bound
on
the
complexity
of
an
algorithm
o(â·
p
221
the
odds
of
an
event
p
p
155
precision
p(â·
p
220
probability
p
p
465
transition
probability
matrix
q
p
59
a
query
r
p
155
recall
si
p
58
a
string
si
p
112
boolean
values
for
zone
scoring
sim(d1
d2
p
121
similarity
score
for
documents
d1
d2
t
p
43
total
number
of
tokens
in
the
document
collection
tct
p
259
number
of
occurrences
of
word
t
in
documents
of
class
c
t
p
4
index
of
the
t
th
term
in
the
vocabulary
v
t
p
61
a
term
in
the
vocabulary
tft,d
p
117
the
term
frequency
of
term
t
in
document
d
the
total
number
of
occurrences
of
t
in
d
ut
p
266
random
variable
taking
values
0
term
t
is
present
and
1
t
is
not
present
v
p
208
vocabulary
of
terms
{t1





tm}
in
a
collection
a.k.a
the
lexicon
~v(d
p
122
length-normalized
document
vector
v~
d
p
120
vector
of
document
d
not
length-normalized
wft,d
p
125
weight
of
term
t
in
document
d
w
p
112
a
weight
for
example
for
zones
or
terms
w~
t~x
=
b
p
293
hyperplane
w~
is
the
normal
vector
of
the
hyperplane
and
wi
component
i
of
w~
~x
p
222
term
incidence
vector
~x
=
x1




xm
more
generally
document
feature
representation
x
p
266
random
variable
taking
values
in
v
the
vocabulary
e.g
at
a
given
position
k
in
a
document
x
p
256
document
space
in
text
classification
|a|
p
61
set
cardinality
the
number
of
members
of
set
a
|s|
p
404
determinant
of
the
square
matrix
s
online
edition
c
2009
cambridge
up
xxx
table
of
notation
|si
|
p
58
length
in
characters
of
string
si
|~x|
p
139
length
of
vector
~x
|~x
âˆ’~y|
p
131
euclidean
distance
of
~x
and
~y
which
is
the
length
of
~x
âˆ’~y
online
edition
c
2009
cambridge
up
draft
â©
april
1
2009
cambridge
university
press
feedback
welcome
xxxi
preface
as
recently
as
the
1990s
studies
showed
that
most
people
preferred
getting
information
from
other
people
rather
than
from
information
retrieval
systems
of
course
in
that
time
period
most
people
also
used
human
travel
agents
to
book
their
travel
however
during
the
last
decade
relentless
optimization
of
information
retrieval
effectiveness
has
driven
web
search
engines
to
new
quality
levels
where
most
people
are
satisfied
most
of
the
time
and
web
search
has
become
a
standard
and
often
preferred
source
of
information
finding
for
example
the
2004
pew
internet
survey
fallows
2004
found
that
â€œ92%
of
internet
users
say
the
internet
is
a
good
place
to
go
for
getting
everyday
information.â€
to
the
surprise
of
many
the
field
of
information
retrieval
has
moved
from
being
a
primarily
academic
discipline
to
being
the
basis
underlying
most
peopleâ€™s
preferred
means
of
information
access
this
book
presents
the
scientific
underpinnings
of
this
field
at
a
level
accessible
to
graduate
students
as
well
as
advanced
undergraduates
information
retrieval
did
not
begin
with
the
web
in
response
to
various
challenges
of
providing
information
access
the
field
of
information
retrieval
evolved
to
give
principled
approaches
to
searching
various
forms
of
content
the
field
began
with
scientific
publications
and
library
records
but
soon
spread
to
other
forms
of
content
particularly
those
of
information
professionals
such
as
journalists
lawyers
and
doctors
much
of
the
scientific
research
on
information
retrieval
has
occurred
in
these
contexts
and
much
of
the
continued
practice
of
information
retrieval
deals
with
providing
access
to
unstructured
information
in
various
corporate
and
governmental
domains
and
this
work
forms
much
of
the
foundation
of
our
book
nevertheless
in
recent
years
a
principal
driver
of
innovation
has
been
the
world
wide
web
unleashing
publication
at
the
scale
of
tens
of
millions
of
content
creators
this
explosion
of
published
information
would
be
moot
if
the
information
could
not
be
found
annotated
and
analyzed
so
that
each
user
can
quickly
find
information
that
is
both
relevant
and
comprehensive
for
their
needs
by
the
late
1990s
many
people
felt
that
continuing
to
index
online
edition
c
2009
cambridge
up
xxxii
preface
the
whole
web
would
rapidly
become
impossible
due
to
the
webâ€™s
exponential
growth
in
size
but
major
scientific
innovations
superb
engineering
the
rapidly
declining
price
of
computer
hardware
and
the
rise
of
a
commercial
underpinning
for
web
search
have
all
conspired
to
power
todayâ€™s
major
search
engines
which
are
able
to
provide
high-quality
results
within
subsecond
response
times
for
hundreds
of
millions
of
searches
a
day
over
billions
of
web
pages
book
organization
and
course
development
this
book
is
the
result
of
a
series
of
courses
we
have
taught
at
stanford
university
and
at
the
university
of
stuttgart
in
a
range
of
durations
including
a
single
quarter
one
semester
and
two
quarters
these
courses
were
aimed
at
early-stage
graduate
students
in
computer
science
but
we
have
also
had
enrollment
from
upper-class
computer
science
undergraduates
as
well
as
students
from
law
medical
informatics
statistics
linguistics
and
various
engineering
disciplines
the
key
design
principle
for
this
book
therefore
was
to
cover
what
we
believe
to
be
important
in
a
one-term
graduate
course
on
information
retrieval
an
additional
principle
is
to
build
each
chapter
around
material
that
we
believe
can
be
covered
in
a
single
lecture
of
75
to
90
minutes
the
first
eight
chapters
of
the
book
are
devoted
to
the
basics
of
information
retrieval
and
in
particular
the
heart
of
search
engines
we
consider
this
material
to
be
core
to
any
course
on
information
retrieval
chapter
1
introduces
inverted
indexes
and
shows
how
simple
boolean
queries
can
be
processed
using
such
indexes
chapter
2
builds
on
this
introduction
by
detailing
the
manner
in
which
documents
are
preprocessed
before
indexing
and
by
discussing
how
inverted
indexes
are
augmented
in
various
ways
for
functionality
and
speed
chapter
3
discusses
search
structures
for
dictionaries
and
how
to
process
queries
that
have
spelling
errors
and
other
imprecise
matches
to
the
vocabulary
in
the
document
collection
being
searched
chapter
4
describes
a
number
of
algorithms
for
constructing
the
inverted
index
from
a
text
collection
with
particular
attention
to
highly
scalable
and
distributed
algorithms
that
can
be
applied
to
very
large
collections
chapter
5
covers
techniques
for
compressing
dictionaries
and
inverted
indexes
these
techniques
are
critical
for
achieving
subsecond
response
times
to
user
queries
in
large
search
engines
the
indexes
and
queries
considered
in
chapters
1â€“5
only
deal
with
boolean
retrieval
in
which
a
document
either
matches
a
query
or
does
not
a
desire
to
measure
the
extent
to
which
a
document
matches
a
query
or
the
score
of
a
document
for
a
query
motivates
the
development
of
term
weighting
and
the
computation
of
scores
in
chapters
6
and
7
leading
to
the
idea
of
a
list
of
documents
that
are
rank-ordered
for
a
query
chapter
8
focuses
on
the
evaluation
of
an
information
retrieval
system
based
on
the
online
edition
c
2009
cambridge
up
preface
xxxiii
relevance
of
the
documents
it
retrieves
allowing
us
to
compare
the
relative
performances
of
different
systems
on
benchmark
document
collections
and
queries
chapters
9â€“21
build
on
the
foundation
of
the
first
eight
chapters
to
cover
a
variety
of
more
advanced
topics
chapter
9
discusses
methods
by
which
retrieval
can
be
enhanced
through
the
use
of
techniques
like
relevance
feedback
and
query
expansion
which
aim
at
increasing
the
likelihood
of
retrieving
relevant
documents
chapter
10
considers
information
retrieval
from
documents
that
are
structured
with
markup
languages
like
xml
and
html
we
treat
structured
retrieval
by
reducing
it
to
the
vector
space
scoring
methods
developed
in
chapter
6
chapters
11
and
12
invoke
probability
theory
to
compute
scores
for
documents
on
queries
chapter
11
develops
traditional
probabilistic
information
retrieval
which
provides
a
framework
for
computing
the
probability
of
relevance
of
a
document
given
a
set
of
query
terms
this
probability
may
then
be
used
as
a
score
in
ranking
chapter
12
illustrates
an
alternative
wherein
for
each
document
in
a
collection
we
build
a
language
model
from
which
one
can
estimate
a
probability
that
the
language
model
generates
a
given
query
this
probability
is
another
quantity
with
which
we
can
rank-order
documents
chapters
13â€“17
give
a
treatment
of
various
forms
of
machine
learning
and
numerical
methods
in
information
retrieval
chapters
13â€“15
treat
the
problem
of
classifying
documents
into
a
set
of
known
categories
given
a
set
of
documents
along
with
the
classes
they
belong
to
chapter
13
motivates
statistical
classification
as
one
of
the
key
technologies
needed
for
a
successful
search
engine
introduces
naive
bayes
a
conceptually
simple
and
efficient
text
classification
method
and
outlines
the
standard
methodology
for
evaluating
text
classifiers
chapter
14
employs
the
vector
space
model
from
chapter
6
and
introduces
two
classification
methods
rocchio
and
knn
that
operate
on
document
vectors
it
also
presents
the
bias-variance
tradeoff
as
an
important
characterization
of
learning
problems
that
provides
criteria
for
selecting
an
appropriate
method
for
a
text
classification
problem
chapter
15
introduces
support
vector
machines
which
many
researchers
currently
view
as
the
most
effective
text
classification
method
we
also
develop
connections
in
this
chapter
between
the
problem
of
classification
and
seemingly
disparate
topics
such
as
the
induction
of
scoring
functions
from
a
set
of
training
examples
chapters
16â€“18
consider
the
problem
of
inducing
clusters
of
related
documents
from
a
collection
in
chapter
16
we
first
give
an
overview
of
a
number
of
important
applications
of
clustering
in
information
retrieval
we
then
describe
two
flat
clustering
algorithms
the
k-means
algorithm
an
efficient
and
widely
used
document
clustering
method
and
the
expectationmaximization
algorithm
which
is
computationally
more
expensive
but
also
more
flexible
chapter
17
motivates
the
need
for
hierarchically
structured
online
edition
c
2009
cambridge
up
xxxiv
preface
clusterings
instead
of
flat
clusterings
in
many
applications
in
information
retrieval
and
introduces
a
number
of
clustering
algorithms
that
produce
a
hierarchy
of
clusters
the
chapter
also
addresses
the
difficult
problem
of
automatically
computing
labels
for
clusters
chapter
18
develops
methods
from
linear
algebra
that
constitute
an
extension
of
clustering
and
also
offer
intriguing
prospects
for
algebraic
methods
in
information
retrieval
which
have
been
pursued
in
the
approach
of
latent
semantic
indexing
chapters
19â€“21
treat
the
problem
of
web
search
we
give
in
chapter
19
a
summary
of
the
basic
challenges
in
web
search
together
with
a
set
of
techniques
that
are
pervasive
in
web
information
retrieval
next
chapter
20
describes
the
architecture
and
requirements
of
a
basic
web
crawler
finally
chapter
21
considers
the
power
of
link
analysis
in
web
search
using
in
the
process
several
methods
from
linear
algebra
and
advanced
probability
theory
this
book
is
not
comprehensive
in
covering
all
topics
related
to
information
retrieval
we
have
put
aside
a
number
of
topics
which
we
deemed
outside
the
scope
of
what
we
wished
to
cover
in
an
introduction
to
information
retrieval
class
nevertheless
for
people
interested
in
these
topics
we
provide
a
few
pointers
to
mainly
textbook
coverage
here
cross-language
ir
grossman
and
frieder
2004
ch
4
and
oard
and
dorr
1996
image
and
multimedia
ir
grossman
and
frieder
2004
ch
4
baeza-yates
and
ribeiro-neto
1999
ch
6
baeza-yates
and
ribeiro-neto
1999
ch
11
baeza-yates
and
ribeiro-neto
1999
ch
12
del
bimbo
1999
lew
2001
and
smeulders
et
al
2000
speech
retrieval
coden
et
al
2002
music
retrieval
downie
2006
and
http://www.ismir.net/
user
interfaces
for
ir
baeza-yates
and
ribeiro-neto
1999
ch
10
parallel
and
peer-to-peer
ir
grossman
and
frieder
2004
ch
7
baeza-yates
and
ribeiro-neto
1999
ch
9
and
aberer
2001
digital
libraries
baeza-yates
and
ribeiro-neto
1999
ch
15
and
lesk
2004
information
science
perspective
korfhage
1997
meadow
et
al
1999
and
ingwersen
and
jã¤rvelin
2005
logic-based
approaches
to
ir
van
rijsbergen
1989
natural
language
processing
techniques
manning
and
schã¼tze
1999
jurafsky
and
martin
2008
and
lewis
and
jones
1996
online
edition
c
2009
cambridge
up
preface
xxxv
prerequisites
introductory
courses
in
data
structures
and
algorithms
in
linear
algebra
and
in
probability
theory
suffice
as
prerequisites
for
all
21
chapters
we
now
give
more
detail
for
the
benefit
of
readers
and
instructors
who
wish
to
tailor
their
reading
to
some
of
the
chapters
chapters
1â€“5
assume
as
prerequisite
a
basic
course
in
algorithms
and
data
structures
chapters
6
and
7
require
in
addition
a
knowledge
of
basic
linear
algebra
including
vectors
and
dot
products
no
additional
prerequisites
are
assumed
until
chapter
11
where
a
basic
course
in
probability
theory
is
required
section
11.1
gives
a
quick
review
of
the
concepts
necessary
in
chapters
11â€“13
chapter
15
assumes
that
the
reader
is
familiar
with
the
notion
of
nonlinear
optimization
although
the
chapter
may
be
read
without
detailed
knowledge
of
algorithms
for
nonlinear
optimization
chapter
18
demands
a
first
course
in
linear
algebra
including
familiarity
with
the
notions
of
matrix
rank
and
eigenvectors
a
brief
review
is
given
in
section
18.1
the
knowledge
of
eigenvalues
and
eigenvectors
is
also
necessary
in
chapter
21
book
layout
âœž
worked
examples
in
the
text
appear
with
a
pencil
sign
next
to
them
in
the
left
margin
advanced
or
difficult
material
appears
in
sections
or
subsections
âœ„
indicated
with
scissors
in
the
margin
exercises
are
marked
in
the
margin
with
a
question
mark
the
level
of
difficulty
of
exercises
is
indicated
as
easy
â‹†
medium
â‹†â‹†
or
difficult
â‹†
â‹†
â‹†
?
acknowledgments
we
would
like
to
thank
cambridge
university
press
for
allowing
us
to
make
the
draft
book
available
online
which
facilitated
much
of
the
feedback
we
have
received
while
writing
the
book
we
also
thank
lauren
cowles
who
has
been
an
outstanding
editor
providing
several
rounds
of
comments
on
each
chapter
on
matters
of
style
organization
and
coverage
as
well
as
detailed
comments
on
the
subject
matter
of
the
book
to
the
extent
that
we
have
achieved
our
goals
in
writing
this
book
she
deserves
an
important
part
of
the
credit
we
are
very
grateful
to
the
many
people
who
have
given
us
comments
suggestions
and
corrections
based
on
draft
versions
of
this
book
we
thank
for
providing
various
corrections
and
comments
cheryl
aasheim
josh
attenberg
daniel
beck
luc
bã©langer
georg
buscher
tom
breuel
daniel
burckhardt
fazli
can
dinquan
chen
stephen
clark
ernest
davis
pedro
domingos
rodrigo
panchiniak
fernandes
paolo
ferragina
alex
fraser
norbert
online
edition
c
2009
cambridge
up
xxxvi
preface
fuhr
vignesh
ganapathy
elmer
garduno
xiubo
geng
david
gondek
sergio
govoni
corinna
habets
ben
handy
donna
harman
benjamin
haskell
thomas
hã¼hn
deepak
jain
ralf
jankowitsch
dinakar
jayarajan
vinay
kakade
mei
kobayashi
wessel
kraaij
rick
lafleur
florian
laws
hang
li
david
losada
david
mann
ennio
masi
sven
meyer
zu
eissen
alexander
murzaku
gonzalo
navarro
frank
mccown
paul
mcnamee
christoph
mã¼ller
scott
olsson
tao
qin
megha
raghavan
michal
rosen-zvi
klaus
rothenhã¤usler
kenyu
l
runner
alexander
salamanca
grigory
sapunov
evgeny
shadchnev
tobias
scheffer
nico
schlaefer
ian
soboroff
benno
stein
marcin
sydow
andrew
turner
jason
utt
huey
vo
travis
wade
mike
walsh
changliang
wang
renjing
wang
and
thomas
zeume
many
people
gave
us
detailed
feedback
on
individual
chapters
either
at
our
request
or
through
their
own
initiative
for
this
weâ€™re
particularly
grateful
to
james
allan
omar
alonso
ismail
sengor
altingovde
vo
ngoc
anh
roi
blanco
eric
breck
eric
brown
mark
carman
carlos
castillo
junghoo
cho
aron
culotta
doug
cutting
meghana
deodhar
susan
dumais
johannes
fã¼rnkranz
andreas
heãÿ
djoerd
hiemstra
david
hull
thorsten
joachims
siddharth
jonathan
j
b
jaap
kamps
mounia
lalmas
amy
langville
nicholas
lester
dave
lewis
daniel
lowd
yosi
mass
jeff
michels
alessandro
moschitti
amir
najmi
marc
najork
giorgio
maria
di
nunzio
paul
ogilvie
priyank
patel
jan
pedersen
kathryn
pedings
vassilis
plachouras
daniel
ramage
ghulam
raza
stefan
riezler
michael
schiehlen
helmut
schmid
falk
nicolas
scholer
sabine
schulte
im
walde
fabrizio
sebastiani
sarabjeet
singh
valentin
spitkovsky
alexander
strehl
john
tait
shivakumar
vaithyanathan
ellen
voorhees
gerhard
weikum
dawid
weiss
yiming
yang
yisong
yue
jian
zhang
and
justin
zobel
and
finally
there
were
a
few
reviewers
who
absolutely
stood
out
in
terms
of
the
quality
and
quantity
of
comments
that
they
provided
we
thank
them
for
their
significant
impact
on
the
content
and
structure
of
the
book
we
express
our
gratitude
to
pavel
berkhin
stefan
bã¼ttcher
jamie
callan
byron
dom
torsten
suel
and
andrew
trotman
parts
of
the
initial
drafts
of
chapters
13â€“15
were
based
on
slides
that
were
generously
provided
by
ray
mooney
while
the
material
has
gone
through
extensive
revisions
we
gratefully
acknowledge
rayâ€™s
contribution
to
the
three
chapters
in
general
and
to
the
description
of
the
time
complexities
of
text
classification
algorithms
in
particular
the
above
is
unfortunately
an
incomplete
list
we
are
still
in
the
process
of
incorporating
feedback
we
have
received
and
like
all
opinionated
authors
we
did
not
always
heed
the
advice
that
was
so
freely
given
the
published
versions
of
the
chapters
remain
solely
the
responsibility
of
the
authors
the
authors
thank
stanford
university
and
the
university
of
stuttgart
for
providing
a
stimulating
academic
environment
for
discussing
ideas
and
the
opportunity
to
teach
courses
from
which
this
book
arose
and
in
which
its
online
edition
c
2009
cambridge
up
preface
xxxvii
contents
were
refined
cm
thanks
his
family
for
the
many
hours
theyâ€™ve
let
him
spend
working
on
this
book
and
hopes
heâ€™ll
have
a
bit
more
free
time
on
weekends
next
year
pr
thanks
his
family
for
their
patient
support
through
the
writing
of
this
book
and
is
also
grateful
to
yahoo
inc
for
providing
a
fertile
environment
in
which
to
work
on
this
book
hs
would
like
to
thank
his
parents
family
and
friends
for
their
support
while
writing
this
book
web
and
contact
information
this
book
has
a
companion
website
at
http://informationretrieval.org
as
well
as
links
to
some
more
general
resources
it
is
our
intent
to
maintain
on
this
website
a
set
of
slides
for
each
chapter
which
may
be
used
for
the
corresponding
lecture
we
gladly
welcome
further
feedback
corrections
and
suggestions
on
the
book
which
may
be
sent
to
all
the
authors
at
informationretrieval
at
yahoogroups
dot
com
online
edition
c
2009
cambridge
up
draft
â©
april
1
2009
cambridge
university
press
feedback
welcome
1
1
boolean
retrieval
the
meaning
of
the
term
information
retrieval
can
be
very
broad
just
getting
a
credit
card
out
of
your
wallet
so
that
you
can
type
in
the
card
number
is
a
form
of
information
retrieval
however
as
an
academic
field
of
study
information
information
retrieval
might
be
defined
thus
retrieval
information
retrieval
ir
is
finding
material
usually
documents
of
an
unstructured
nature
usually
text
that
satisfies
an
information
need
from
within
large
collections
usually
stored
on
computers
as
defined
in
this
way
information
retrieval
used
to
be
an
activity
that
only
a
few
people
engaged
in
reference
librarians
paralegals
and
similar
professional
searchers
now
the
world
has
changed
and
hundreds
of
millions
of
people
engage
in
information
retrieval
every
day
when
they
use
a
web
search
engine
or
search
their
email.1
information
retrieval
is
fast
becoming
the
dominant
form
of
information
access
overtaking
traditional
databasestyle
searching
the
sort
that
is
going
on
when
a
clerk
says
to
you
â€œiâ€™m
sorry
i
can
only
look
up
your
order
if
you
can
give
me
your
order
idâ€
ir
can
also
cover
other
kinds
of
data
and
information
problems
beyond
that
specified
in
the
core
definition
above
the
term
â€œunstructured
dataâ€
refers
to
data
which
does
not
have
clear
semantically
overt
easy-for-a-computer
structure
it
is
the
opposite
of
structured
data
the
canonical
example
of
which
is
a
relational
database
of
the
sort
companies
usually
use
to
maintain
product
inventories
and
personnel
records
in
reality
almost
no
data
are
truly
â€œunstructuredâ€
this
is
definitely
true
of
all
text
data
if
you
count
the
latent
linguistic
structure
of
human
languages
but
even
accepting
that
the
intended
notion
of
structure
is
overt
structure
most
text
has
structure
such
as
headings
and
paragraphs
and
footnotes
which
is
commonly
represented
in
documents
by
explicit
markup
such
as
the
coding
underlying
web
1
in
modern
parlance
the
word
â€œsearchâ€
has
tended
to
replace
â€œ(information
retrievalâ€
the
term
â€œsearchâ€
is
quite
ambiguous
but
in
context
we
use
the
two
synonymously
online
edition
c
2009
cambridge
up
2
1
boolean
retrieval
pages
ir
is
also
used
to
facilitate
â€œsemistructuredâ€
search
such
as
finding
a
document
where
the
title
contains
java
and
the
body
contains
threading
the
field
of
information
retrieval
also
covers
supporting
users
in
browsing
or
filtering
document
collections
or
further
processing
a
set
of
retrieved
documents
given
a
set
of
documents
clustering
is
the
task
of
coming
up
with
a
good
grouping
of
the
documents
based
on
their
contents
it
is
similar
to
arranging
books
on
a
bookshelf
according
to
their
topic
given
a
set
of
topics
standing
information
needs
or
other
categories
such
as
suitability
of
texts
for
different
age
groups
classification
is
the
task
of
deciding
which
class(es
if
any
each
of
a
set
of
documents
belongs
to
it
is
often
approached
by
first
manually
classifying
some
documents
and
then
hoping
to
be
able
to
classify
new
documents
automatically
information
retrieval
systems
can
also
be
distinguished
by
the
scale
at
which
they
operate
and
it
is
useful
to
distinguish
three
prominent
scales
in
web
search
the
system
has
to
provide
search
over
billions
of
documents
stored
on
millions
of
computers
distinctive
issues
are
needing
to
gather
documents
for
indexing
being
able
to
build
systems
that
work
efficiently
at
this
enormous
scale
and
handling
particular
aspects
of
the
web
such
as
the
exploitation
of
hypertext
and
not
being
fooled
by
site
providers
manipulating
page
content
in
an
attempt
to
boost
their
search
engine
rankings
given
the
commercial
importance
of
the
web
we
focus
on
all
these
issues
in
chapters
19â€“21
at
the
other
extreme
is
personal
information
retrieval
in
the
last
few
years
consumer
operating
systems
have
integrated
information
retrieval
such
as
appleâ€™s
mac
os
x
spotlight
or
windows
vistaâ€™s
instant
search
email
programs
usually
not
only
provide
search
but
also
text
classification
they
at
least
provide
a
spam
junk
mail
filter
and
commonly
also
provide
either
manual
or
automatic
means
for
classifying
mail
so
that
it
can
be
placed
directly
into
particular
folders
distinctive
issues
here
include
handling
the
broad
range
of
document
types
on
a
typical
personal
computer
and
making
the
search
system
maintenance
free
and
sufficiently
lightweight
in
terms
of
startup
processing
and
disk
space
usage
that
it
can
run
on
one
machine
without
annoying
its
owner
in
between
is
the
space
of
enterprise
institutional
and
domain-specific
search
where
retrieval
might
be
provided
for
collections
such
as
a
corporationâ€™s
internal
documents
a
database
of
patents
or
research
articles
on
biochemistry
in
this
case
the
documents
will
typically
be
stored
on
centralized
file
systems
and
one
or
a
handful
of
dedicated
machines
will
provide
search
over
the
collection
this
book
contains
techniques
of
value
over
this
whole
spectrum
but
our
coverage
of
some
aspects
of
parallel
and
distributed
search
in
web-scale
search
systems
is
comparatively
light
owing
to
the
relatively
small
published
literature
on
the
details
of
such
systems
however
outside
of
a
handful
of
web
search
companies
a
software
developer
is
most
likely
to
encounter
the
personal
search
and
enterprise
scenarios
online
edition
c
2009
cambridge
up
1.1
an
example
information
retrieval
problem
3
in
this
chapter
we
begin
with
a
very
simple
example
of
an
information
retrieval
problem
and
introduce
the
idea
of
a
term-document
matrix
section
1.1
and
the
central
inverted
index
data
structure
section
1.2
we
will
then
examine
the
boolean
retrieval
model
and
how
boolean
queries
are
processed
sections
1.3
and
1.4
1.1
an
example
information
retrieval
problem
a
fat
book
which
many
people
own
is
shakespeareâ€™s
collected
works
suppose
you
wanted
to
determine
which
plays
of
shakespeare
contain
the
words
brutus
and
caesar
and
not
calpurnia
one
way
to
do
that
is
to
start
at
the
beginning
and
to
read
through
all
the
text
noting
for
each
play
whether
it
contains
brutus
and
caesar
and
excluding
it
from
consideration
if
it
contains
calpurnia
the
simplest
form
of
document
retrieval
is
for
a
computer
to
do
this
sort
of
linear
scan
through
documents
this
process
is
commonly
grep
referred
to
as
grepping
through
text
after
the
unix
command
grep
which
performs
this
process
grepping
through
text
can
be
a
very
effective
process
especially
given
the
speed
of
modern
computers
and
often
allows
useful
possibilities
for
wildcard
pattern
matching
through
the
use
of
regular
expressions
with
modern
computers
for
simple
querying
of
modest
collections
the
size
of
shakespeareâ€™s
collected
works
is
a
bit
under
one
million
words
of
text
in
total
you
really
need
nothing
more
but
for
many
purposes
you
do
need
more
1
to
process
large
document
collections
quickly
the
amount
of
online
data
has
grown
at
least
as
quickly
as
the
speed
of
computers
and
we
would
now
like
to
be
able
to
search
collections
that
total
in
the
order
of
billions
to
trillions
of
words
2
to
allow
more
flexible
matching
operations
for
example
it
is
impractical
to
perform
the
query
romans
near
countrymen
with
grep
where
near
might
be
defined
as
â€œwithin
5
wordsâ€
or
â€œwithin
the
same
sentenceâ€
3
to
allow
ranked
retrieval
in
many
cases
you
want
the
best
answer
to
an
information
need
among
many
documents
that
contain
certain
words
index
the
way
to
avoid
linearly
scanning
the
texts
for
each
query
is
to
index
the
documents
in
advance
let
us
stick
with
shakespeareâ€™s
collected
works
and
use
it
to
introduce
the
basics
of
the
boolean
retrieval
model
suppose
we
record
for
each
document
â€“
here
a
play
of
shakespeareâ€™s
â€“
whether
it
contains
each
word
out
of
all
the
words
shakespeare
used
shakespeare
used
incidence
matrix
about
32,000
different
words
the
result
is
a
binary
term-document
incidence
term
matrix
as
in
figure
1.1
terms
are
the
indexed
units
further
discussed
in
section
2.2
they
are
usually
words
and
for
the
moment
you
can
think
of
online
edition
c
2009
cambridge
up
4
1
boolean
retrieval
antony
julius
the
hamlet
othello
macbeth



and
caesar
tempest
cleopatra
antony
1
1
0
0
0
1
brutus
1
1
0
1
0
0
caesar
1
1
0
1
1
1
calpurnia
0
1
0
0
0
0
cleopatra
1
0
0
0
0
0
mercy
1
0
1
1
1
1
worser
1
0
1
1
1
0



â—®
figure
1.1
a
term-document
incidence
matrix
matrix
element
t
d
is
1
if
the
play
in
column
d
contains
the
word
in
row
t
and
is
0
otherwise
them
as
words
but
the
information
retrieval
literature
normally
speaks
of
terms
because
some
of
them
such
as
perhaps
i-9
or
hong
kong
are
not
usually
thought
of
as
words
now
depending
on
whether
we
look
at
the
matrix
rows
or
columns
we
can
have
a
vector
for
each
term
which
shows
the
documents
it
appears
in
or
a
vector
for
each
document
showing
the
terms
that
occur
in
it.2
to
answer
the
query
brutus
and
caesar
and
not
calpurnia
we
take
the
vectors
for
brutus
caesar
and
calpurnia
complement
the
last
and
then
do
a
bitwise
and
110100
and
110111
and
101111
=
100100
the
answers
for
this
query
are
thus
antony
and
cleopatra
and
hamlet
figure
1.2
boolean
retrieval
the
boolean
retrieval
model
is
a
model
for
information
retrieval
in
which
we
model
can
pose
any
query
which
is
in
the
form
of
a
boolean
expression
of
terms
that
is
in
which
terms
are
combined
with
the
operators
and
or
and
not
the
model
views
each
document
as
just
a
set
of
words
let
us
now
consider
a
more
realistic
scenario
simultaneously
using
the
opportunity
to
introduce
some
terminology
and
notation
suppose
we
have
document
n
=
1
million
documents
by
documents
we
mean
whatever
units
we
have
decided
to
build
a
retrieval
system
over
they
might
be
individual
memos
or
chapters
of
a
book
see
section
2.1.2
page
20
for
further
discussion
we
will
refer
to
the
group
of
documents
over
which
we
perform
retrieval
as
the
collection
document
collection
it
is
sometimes
also
referred
to
as
a
corpus
a
body
of
corpus
texts
suppose
each
document
is
about
1000
words
long
2â€“3
book
pages
if
2
formally
we
take
the
transpose
of
the
matrix
to
be
able
to
get
the
terms
as
column
vectors
online
edition
c
2009
cambridge
up
1.1
an
example
information
retrieval
problem
5
antony
and
cleopatra
act
iii
scene
ii
agrippa
aside
to
domitius
enobarbus
why
enobarbus
when
antony
found
julius
caesar
dead
he
cried
almost
to
roaring
and
he
wept
when
at
philippi
he
found
brutus
slain
hamlet
act
iii
scene
ii
lord
polonius
i
did
enact
julius
caesar
i
was
killed
iâ€™
the
capitol
brutus
killed
me
â—®
figure
1.2
results
from
shakespeare
for
the
query
brutus
and
caesar
and
not
calpurnia
we
assume
an
average
of
6
bytes
per
word
including
spaces
and
punctuation
then
this
is
a
document
collection
about
6
gb
in
size
typically
there
might
be
about
m
=
500,000
distinct
terms
in
these
documents
there
is
nothing
special
about
the
numbers
we
have
chosen
and
they
might
vary
by
an
order
of
magnitude
or
more
but
they
give
us
some
idea
of
the
dimensions
of
the
kinds
of
problems
we
need
to
handle
we
will
discuss
and
model
these
size
assumptions
in
section
5.1
page
86
ad
hoc
retrieval
our
goal
is
to
develop
a
system
to
address
the
ad
hoc
retrieval
task
this
is
the
most
standard
ir
task
in
it
a
system
aims
to
provide
documents
from
within
the
collection
that
are
relevant
to
an
arbitrary
user
information
need
communicated
to
the
system
by
means
of
a
one-off
user-initiated
query
an
information
need
information
need
is
the
topic
about
which
the
user
desires
to
know
more
and
query
is
differentiated
from
a
query
which
is
what
the
user
conveys
to
the
computer
in
an
attempt
to
communicate
the
information
need
a
document
is
relevance
relevant
if
it
is
one
that
the
user
perceives
as
containing
information
of
value
with
respect
to
their
personal
information
need
our
example
above
was
rather
artificial
in
that
the
information
need
was
defined
in
terms
of
particular
words
whereas
usually
a
user
is
interested
in
a
topic
like
â€œpipeline
leaksâ€
and
would
like
to
find
relevant
documents
regardless
of
whether
they
precisely
use
those
words
or
express
the
concept
with
other
words
such
as
effectiveness
pipeline
rupture
to
assess
the
effectiveness
of
an
ir
system
i.e
the
quality
of
its
search
results
a
user
will
usually
want
to
know
two
key
statistics
about
the
systemâ€™s
returned
results
for
a
query
precision
precision
what
fraction
of
the
returned
results
are
relevant
to
the
information
need?
recall
recall
what
fraction
of
the
relevant
documents
in
the
collection
were
returned
by
the
system?
online
edition
c
2009
cambridge
up
6
1
boolean
retrieval
detailed
discussion
of
relevance
and
evaluation
measures
including
precision
and
recall
is
found
in
chapter
8
we
now
cannot
build
a
term-document
matrix
in
a
naive
way
a
500k
ã—
1m
matrix
has
half-a-trillion
0â€™s
and
1â€™s
â€“
too
many
to
fit
in
a
computerâ€™s
memory
but
the
crucial
observation
is
that
the
matrix
is
extremely
sparse
that
is
it
has
few
non-zero
entries
because
each
document
is
1000
words
long
the
matrix
has
no
more
than
one
billion
1â€™s
so
a
minimum
of
99.8%
of
the
cells
are
zero
a
much
better
representation
is
to
record
only
the
things
that
do
occur
that
is
the
1
positions
this
idea
is
central
to
the
first
major
concept
in
information
retrieval
the
inverted
index
inverted
index
the
name
is
actually
redundant
an
index
always
maps
back
from
terms
to
the
parts
of
a
document
where
they
occur
nevertheless
inverted
index
or
sometimes
inverted
file
has
become
the
standard
term
in
information
retrieval.3
the
basic
idea
of
an
inverted
index
is
shown
in
figure
1.3
dictionary
we
keep
a
dictionary
of
terms
sometimes
also
referred
to
as
a
vocabulary
or
vocabulary
lexicon
in
this
book
we
use
dictionary
for
the
data
structure
and
vocabulary
lexicon
for
the
set
of
terms
then
for
each
term
we
have
a
list
that
records
which
documents
the
term
occurs
in
each
item
in
the
list
â€“
which
records
that
a
term
appeared
in
a
document
and
later
often
the
positions
in
the
document
â€“
is
conventionally
called
a
posting
4
posting
the
list
is
then
called
a
postings
postings
list
list
or
inverted
list
and
all
the
postings
lists
taken
together
are
referred
to
as
postings
the
postings
the
dictionary
in
figure
1.3
has
been
sorted
alphabetically
and
each
postings
list
is
sorted
by
document
id
we
will
see
why
this
is
useful
in
section
1.3
below
but
later
we
will
also
consider
alternatives
to
doing
this
section
7.1.5
1.2
a
first
take
at
building
an
inverted
index
to
gain
the
speed
benefits
of
indexing
at
retrieval
time
we
have
to
build
the
index
in
advance
the
major
steps
in
this
are
1
collect
the
documents
to
be
indexed
friends
romans
countrymen
so
let
it
be
with
caesar



2
tokenize
the
text
turning
each
document
into
a
list
of
tokens
friends
romans
countrymen
so



3
some
information
retrieval
researchers
prefer
the
term
inverted
file
but
expressions
like
index
construction
and
index
compression
are
much
more
common
than
inverted
file
construction
and
inverted
file
compression
for
consistency
we
use
inverted
index
throughout
this
book
4
in
a
non-positional
inverted
index
a
posting
is
just
a
document
id
but
it
is
inherently
associated
with
a
term
via
the
postings
list
it
is
placed
on
sometimes
we
will
also
talk
of
a
term
docid
pair
as
a
posting
online
edition
c
2009
cambridge
up
1.2
a
first
take
at
building
an
inverted
index
7
brutus
âˆ’â†’
1
2
4
11
31
45
173
174
caesar
âˆ’â†’
1
2
4
5
6
16
57
132



calpurnia
âˆ’â†’
2
31
54
101



|
{z
}
|
{z
}
dictionary
postings
â—®
figure
1.3
the
two
parts
of
an
inverted
index
the
dictionary
is
commonly
kept
in
memory
with
pointers
to
each
postings
list
which
is
stored
on
disk
3
do
linguistic
preprocessing
producing
a
list
of
normalized
tokens
which
are
the
indexing
terms
friend
roman
countryman
so



4
index
the
documents
that
each
term
occurs
in
by
creating
an
inverted
index
consisting
of
a
dictionary
and
postings
we
will
define
and
discuss
the
earlier
stages
of
processing
that
is
steps
1â€“3
in
section
2.2
page
22
until
then
you
can
think
of
tokens
and
normalized
tokens
as
also
loosely
equivalent
to
words
here
we
assume
that
the
first
3
steps
have
already
been
done
and
we
examine
building
a
basic
inverted
index
by
sort-based
indexing
within
a
document
collection
we
assume
that
each
document
has
a
unique
docid
serial
number
known
as
the
document
identifier
docid
during
index
construction
we
can
simply
assign
successive
integers
to
each
new
document
when
it
is
first
encountered
the
input
to
indexing
is
a
list
of
normalized
tokens
for
each
document
which
we
can
equally
think
of
as
a
list
of
pairs
of
sorting
term
and
docid
as
in
figure
1.4
the
core
indexing
step
is
sorting
this
list
so
that
the
terms
are
alphabetical
giving
us
the
representation
in
the
middle
column
of
figure
1.4
multiple
occurrences
of
the
same
term
from
the
same
document
are
then
merged.5
instances
of
the
same
term
are
then
grouped
and
the
result
is
split
into
a
dictionary
and
postings
as
shown
in
the
right
column
of
figure
1.4
since
a
term
generally
occurs
in
a
number
of
documents
this
data
organization
already
reduces
the
storage
requirements
of
the
index
the
dictionary
also
records
some
statistics
such
as
the
number
of
document
documents
which
contain
each
term
the
document
frequency
which
is
here
frequency
also
the
length
of
each
postings
list
this
information
is
not
vital
for
a
basic
boolean
search
engine
but
it
allows
us
to
improve
the
efficiency
of
the
5
unix
users
can
note
that
these
steps
are
similar
to
use
of
the
sort
and
then
uniq
commands
online
edition
c
2009
cambridge
up
8
1
boolean
retrieval
doc
1
doc
2
i
did
enact
julius
caesar
i
was
killed
iâ€™
the
capitol
brutus
killed
me
so
let
it
be
with
caesar
the
noble
brutus
hath
told
you
caesar
was
ambitious
term
docid
i
1
did
1
enact
1
julius
1
caesar
1
i
1
was
1
killed
1
iâ€™
1
the
1
capitol
1
brutus
1
killed
1
me
1
so
2
let
2
it
2
be
2
with
2
caesar
2
the
2
noble
2
brutus
2
hath
2
told
2
you
2
caesar
2
was
2
ambitious
2
=â‡’
term
docid
ambitious
2
be
2
brutus
1
brutus
2
capitol
1
caesar
1
caesar
2
caesar
2
did
1
enact
1
hath
1
i
1
i
1
iâ€™
1
it
2
julius
1
killed
1
killed
1
let
2
me
1
noble
2
so
2
the
1
the
2
told
2
you
2
was
1
was
2
with
2
=â‡’
term
doc
freq
â†’
postings
lists
ambitious
1
â†’
2
be
1
â†’
2
brutus
2
â†’
1
â†’
2
capitol
1
â†’
1
caesar
2
â†’
1
â†’
2
did
1
â†’
1
enact
1
â†’
1
hath
1
â†’
2
i
1
â†’
1
iâ€™
1
â†’
1
it
1
â†’
2
julius
1
â†’
1
killed
1
â†’
1
let
1
â†’
2
me
1
â†’
1
noble
1
â†’
2
so
1
â†’
2
the
2
â†’
1
â†’
2
told
1
â†’
2
you
1
â†’
2
was
2
â†’
1
â†’
2
with
1
â†’
2
â—®
figure
1.4
building
an
index
by
sorting
and
grouping
the
sequence
of
terms
in
each
document
tagged
by
their
documentid
left
is
sorted
alphabetically
middle
instances
of
the
same
term
are
then
grouped
by
word
and
then
by
documentid
the
terms
and
documentids
are
then
separated
out
right
the
dictionary
stores
the
terms
and
has
a
pointer
to
the
postings
list
for
each
term
it
commonly
also
stores
other
summary
information
such
as
here
the
document
frequency
of
each
term
we
use
this
information
for
improving
query
time
efficiency
and
later
for
weighting
in
ranked
retrieval
models
each
postings
list
stores
the
list
of
documents
in
which
a
term
occurs
and
may
store
other
information
such
as
the
term
frequency
the
frequency
of
each
term
in
each
document
or
the
position(s
of
the
term
in
each
document
online
edition
c
2009
cambridge
up
1.2
a
first
take
at
building
an
inverted
index
9
search
engine
at
query
time
and
it
is
a
statistic
later
used
in
many
ranked
retrieval
models
the
postings
are
secondarily
sorted
by
docid
this
provides
the
basis
for
efficient
query
processing
this
inverted
index
structure
is
essentially
without
rivals
as
the
most
efficient
structure
for
supporting
ad
hoc
text
search
in
the
resulting
index
we
pay
for
storage
of
both
the
dictionary
and
the
postings
lists
the
latter
are
much
larger
but
the
dictionary
is
commonly
kept
in
memory
while
postings
lists
are
normally
kept
on
disk
so
the
size
of
each
is
important
and
in
chapter
5
we
will
examine
how
each
can
be
optimized
for
storage
and
access
efficiency
what
data
structure
should
be
used
for
a
postings
list?
a
fixed
length
array
would
be
wasteful
as
some
words
occur
in
many
documents
and
others
in
very
few
for
an
in-memory
postings
list
two
good
alternatives
are
singly
linked
lists
or
variable
length
arrays
singly
linked
lists
allow
cheap
insertion
of
documents
into
postings
lists
following
updates
such
as
when
recrawling
the
web
for
updated
documents
and
naturally
extend
to
more
advanced
indexing
strategies
such
as
skip
lists
section
2.3
which
require
additional
pointers
variable
length
arrays
win
in
space
requirements
by
avoiding
the
overhead
for
pointers
and
in
time
requirements
because
their
use
of
contiguous
memory
increases
speed
on
modern
processors
with
memory
caches
extra
pointers
can
in
practice
be
encoded
into
the
lists
as
offsets
if
updates
are
relatively
infrequent
variable
length
arrays
will
be
more
compact
and
faster
to
traverse
we
can
also
use
a
hybrid
scheme
with
a
linked
list
of
fixed
length
arrays
for
each
term
when
postings
lists
are
stored
on
disk
they
are
stored
perhaps
compressed
as
a
contiguous
run
of
postings
without
explicit
pointers
as
in
figure
1.3
so
as
to
minimize
the
size
of
the
postings
list
and
the
number
of
disk
seeks
to
read
a
postings
list
into
memory
?
exercise
1.1
â‹†
draw
the
inverted
index
that
would
be
built
for
the
following
document
collection
see
figure
1.3
for
an
example
doc
1
new
home
sales
top
forecasts
doc
2
home
sales
rise
in
july
doc
3
increase
in
home
sales
in
july
doc
4
july
new
home
sales
rise
exercise
1.2
â‹†
consider
these
documents
doc
1
breakthrough
drug
for
schizophrenia
doc
2
new
schizophrenia
drug
doc
3
new
approach
for
treatment
of
schizophrenia
doc
4
new
hopes
for
schizophrenia
patients
a
draw
the
term-document
incidence
matrix
for
this
document
collection
online
edition
c
2009
cambridge
up
10
1
boolean
retrieval
brutus
âˆ’â†’
1
â†’
2
â†’
4
â†’
11
â†’
31
â†’
45
â†’
173
â†’
174
calpurnia
âˆ’â†’
2
â†’
31
â†’
54
â†’
101
intersection
=â‡’
2
â†’
31
â—®
figure
1.5
intersecting
the
postings
lists
for
brutus
and
calpurnia
from
figure
1.3
b
draw
the
inverted
index
representation
for
this
collection
as
in
figure
1.3
page
7
exercise
1.3
â‹†
for
the
document
collection
shown
in
exercise
1.2
what
are
the
returned
results
for
these
queries
a
schizophrenia
and
drug
b
for
and
not(drug
or
approach
1.3
processing
boolean
queries
how
do
we
process
a
query
using
an
inverted
index
and
the
basic
boolean
simple
conjunctive
retrieval
model?
consider
processing
the
simple
conjunctive
query
queries
1.1
brutus
and
calpurnia
over
the
inverted
index
partially
shown
in
figure
1.3
page
7
we
1
locate
brutus
in
the
dictionary
2
retrieve
its
postings
3
locate
calpurnia
in
the
dictionary
4
retrieve
its
postings
5
intersect
the
two
postings
lists
as
shown
in
figure
1.5
postings
list
the
intersection
operation
is
the
crucial
one
we
need
to
efficiently
intersect
intersection
postings
lists
so
as
to
be
able
to
quickly
find
documents
that
contain
both
postings
merge
terms
this
operation
is
sometimes
referred
to
as
merging
postings
lists
this
slightly
counterintuitive
name
reflects
using
the
term
merge
algorithm
for
a
general
family
of
algorithms
that
combine
multiple
sorted
lists
by
interleaved
advancing
of
pointers
through
each
here
we
are
merging
the
lists
with
a
logical
and
operation
there
is
a
simple
and
effective
method
of
intersecting
postings
lists
using
the
merge
algorithm
see
figure
1.6
we
maintain
pointers
into
both
lists
online
edition
c
2009
cambridge
up
1.3
processing
boolean
queries
11
intersect(p1

p2
1
answer
â†
h
i
2
while
p1
6=
nil
and
p2
6=
nil
3
do
if
docid(p1
=
docid(p2
4
then
add(answer
docid(p1
5
p1
â†
next(p1
6
p2
â†
next(p2
7
else
if
docid(p1
<
docid(p2
8
then
p1
â†
next(p1
9
else
p2
â†
next(p2
10
return
answer
â—®
figure
1.6
algorithm
for
the
intersection
of
two
postings
lists
p1
and
p2
and
walk
through
the
two
postings
lists
simultaneously
in
time
linear
in
the
total
number
of
postings
entries
at
each
step
we
compare
the
docid
pointed
to
by
both
pointers
if
they
are
the
same
we
put
that
docid
in
the
results
list
and
advance
both
pointers
otherwise
we
advance
the
pointer
pointing
to
the
smaller
docid
if
the
lengths
of
the
postings
lists
are
x
and
y
the
intersection
takes
o(x
+
y
operations
formally
the
complexity
of
querying
is
î˜(n
where
n
is
the
number
of
documents
in
the
collection.6
our
indexing
methods
gain
us
just
a
constant
not
a
difference
in
î˜
time
complexity
compared
to
a
linear
scan
but
in
practice
the
constant
is
huge
to
use
this
algorithm
it
is
crucial
that
postings
be
sorted
by
a
single
global
ordering
using
a
numeric
sort
by
docid
is
one
simple
way
to
achieve
this
we
can
extend
the
intersection
operation
to
process
more
complicated
queries
like
1.2
brutus
or
caesar
and
not
calpurnia
query
optimization
query
optimization
is
the
process
of
selecting
how
to
organize
the
work
of
answering
a
query
so
that
the
least
total
amount
of
work
needs
to
be
done
by
the
system
a
major
element
of
this
for
boolean
queries
is
the
order
in
which
postings
lists
are
accessed
what
is
the
best
order
for
query
processing?
consider
a
query
that
is
an
and
of
t
terms
for
instance
1.3
brutus
and
caesar
and
calpurnia
for
each
of
the
t
terms
we
need
to
get
its
postings
then
and
them
together
the
standard
heuristic
is
to
process
terms
in
order
of
increasing
document
6
the
notation
î˜(â·
is
used
to
express
an
asymptotically
tight
bound
on
the
complexity
of
an
algorithm
informally
this
is
often
written
as
o(â·
but
this
notation
really
expresses
an
asymptotic
upper
bound
which
need
not
be
tight
cormen
et
al
1990
online
edition
c
2009
cambridge
up
12
1
boolean
retrieval
intersect(ht1





tni
1
terms
â†
sortbyincreasingfrequency(ht1





tni
2
result
â†
postings(f
irst(terms
3
terms
â†
rest(terms
4
while
terms
6=
nil
and
result
6=
nil
5
do
result
â†
intersect(result
postings(f
irst(terms
6
terms
â†
rest(terms
7
return
result
â—®
figure
1.7
algorithm
for
conjunctive
queries
that
returns
the
set
of
documents
containing
each
term
in
the
input
list
of
terms
frequency
if
we
start
by
intersecting
the
two
smallest
postings
lists
then
all
intermediate
results
must
be
no
bigger
than
the
smallest
postings
list
and
we
are
therefore
likely
to
do
the
least
amount
of
total
work
so
for
the
postings
lists
in
figure
1.3
page
7
we
execute
the
above
query
as
1.4
calpurnia
and
brutus
and
caesar
this
is
a
first
justification
for
keeping
the
frequency
of
terms
in
the
dictionary
it
allows
us
to
make
this
ordering
decision
based
on
in-memory
data
before
accessing
any
postings
list
consider
now
the
optimization
of
more
general
queries
such
as
1.5
madding
or
crowd
and
ignoble
or
strife
and
killed
or
slain
as
before
we
will
get
the
frequencies
for
all
terms
and
we
can
then
conservatively
estimate
the
size
of
each
or
by
the
sum
of
the
frequencies
of
its
disjuncts
we
can
then
process
the
query
in
increasing
order
of
the
size
of
each
disjunctive
term
for
arbitrary
boolean
queries
we
have
to
evaluate
and
temporarily
store
the
answers
for
intermediate
expressions
in
a
complex
expression
however
in
many
circumstances
either
because
of
the
nature
of
the
query
language
or
just
because
this
is
the
most
common
type
of
query
that
users
submit
a
query
is
purely
conjunctive
in
this
case
rather
than
viewing
merging
postings
lists
as
a
function
with
two
inputs
and
a
distinct
output
it
is
more
efficient
to
intersect
each
retrieved
postings
list
with
the
current
intermediate
result
in
memory
where
we
initialize
the
intermediate
result
by
loading
the
postings
list
of
the
least
frequent
term
this
algorithm
is
shown
in
figure
1.7
the
intersection
operation
is
then
asymmetric
the
intermediate
results
list
is
in
memory
while
the
list
it
is
being
intersected
with
is
being
read
from
disk
moreover
the
intermediate
results
list
is
always
at
least
as
short
as
the
other
list
and
in
many
cases
it
is
orders
of
magnitude
shorter
the
postings
online
edition
c
2009
cambridge
up
1.3
processing
boolean
queries
13
intersection
can
still
be
done
by
the
algorithm
in
figure
1.6
but
when
the
difference
between
the
list
lengths
is
very
large
opportunities
to
use
alternative
techniques
open
up
the
intersection
can
be
calculated
in
place
by
destructively
modifying
or
marking
invalid
items
in
the
intermediate
results
list
or
the
intersection
can
be
done
as
a
sequence
of
binary
searches
in
the
long
postings
lists
for
each
posting
in
the
intermediate
results
list
another
possibility
is
to
store
the
long
postings
list
as
a
hashtable
so
that
membership
of
an
intermediate
result
item
can
be
calculated
in
constant
rather
than
linear
or
log
time
however
such
alternative
techniques
are
difficult
to
combine
with
postings
list
compression
of
the
sort
discussed
in
chapter
5
moreover
standard
postings
list
intersection
operations
remain
necessary
when
both
terms
of
a
query
are
very
common
?
exercise
1.4
â‹†
for
the
queries
below
can
we
still
run
through
the
intersection
in
time
o(x
+
y
where
x
and
y
are
the
lengths
of
the
postings
lists
for
brutus
and
caesar?
if
not
what
can
we
achieve?
a
brutus
and
not
caesar
b
brutus
or
not
caesar
exercise
1.5
â‹†
extend
the
postings
merge
algorithm
to
arbitrary
boolean
query
formulas
what
is
its
time
complexity?
for
instance
consider
c
brutus
or
caesar
and
not
antony
or
cleopatra
can
we
always
merge
in
linear
time?
linear
in
what?
can
we
do
better
than
this?
exercise
1.6
â‹†â‹†
we
can
use
distributive
laws
for
and
and
or
to
rewrite
queries
a
show
how
to
rewrite
the
query
in
exercise
1.5
into
disjunctive
normal
form
using
the
distributive
laws
b
would
the
resulting
query
be
more
or
less
efficiently
evaluated
than
the
original
form
of
this
query?
c
is
this
result
true
in
general
or
does
it
depend
on
the
words
and
the
contents
of
the
document
collection?
exercise
1.7
â‹†
recommend
a
query
processing
order
for
d
tangerine
or
trees
and
marmalade
or
skies
and
kaleidoscope
or
eyes
given
the
following
postings
list
sizes
online
edition
c
2009
cambridge
up
14
1
boolean
retrieval
term
postings
size
eyes
213312
kaleidoscope
87009
marmalade
107913
skies
271658
tangerine
46653
trees
316812
exercise
1.8
â‹†
if
the
query
is
e
friends
and
romans
and
not
countrymen
how
could
we
use
the
frequency
of
countrymen
in
evaluating
the
best
query
evaluation
order?
in
particular
propose
a
way
of
handling
negation
in
determining
the
order
of
query
processing
exercise
1.9
â‹†â‹†
for
a
conjunctive
query
is
processing
postings
lists
in
order
of
size
guaranteed
to
be
optimal?
explain
why
it
is
or
give
an
example
where
it
isnâ€™t
exercise
1.10
â‹†â‹†
write
out
a
postings
merge
algorithm
in
the
style
of
figure
1.6
page
11
for
an
x
or
y
query
exercise
1.11
â‹†â‹†
how
should
the
boolean
query
x
and
not
y
be
handled?
why
is
naive
evaluation
of
this
query
normally
very
expensive?
write
out
a
postings
merge
algorithm
that
evaluates
this
query
efficiently
1.4
the
extended
boolean
model
versus
ranked
retrieval
ranked
retrieval
the
boolean
retrieval
model
contrasts
with
ranked
retrieval
models
such
as
the
model
vector
space
model
section
6.3
in
which
users
largely
use
free
text
queries
free
text
queries
that
is
just
typing
one
or
more
words
rather
than
using
a
precise
language
with
operators
for
building
up
query
expressions
and
the
system
decides
which
documents
best
satisfy
the
query
despite
decades
of
academic
research
on
the
advantages
of
ranked
retrieval
systems
implementing
the
boolean
retrieval
model
were
the
main
or
only
search
option
provided
by
large
commercial
information
providers
for
three
decades
until
the
early
1990s
approximately
the
date
of
arrival
of
the
world
wide
web
however
these
systems
did
not
have
just
the
basic
boolean
operations
and
or
and
not
which
we
have
presented
so
far
a
strict
boolean
expression
over
terms
with
an
unordered
results
set
is
too
limited
for
many
of
the
information
needs
that
people
have
and
these
systems
implemented
extended
boolean
retrieval
models
by
incorporating
additional
operators
such
as
term
proximity
operproximity
operator
ators
a
proximity
operator
is
a
way
of
specifying
that
two
terms
in
a
query
online
edition
c
2009
cambridge
up
1.4
the
extended
boolean
model
versus
ranked
retrieval
15
must
occur
close
to
each
other
in
a
document
where
closeness
may
be
measured
by
limiting
the
allowed
number
of
intervening
words
or
by
reference
to
a
structural
unit
such
as
a
sentence
or
paragraph
âœž
example
1.1
commercial
boolean
searching
westlaw
westlaw
http://www.westlaw.com/
is
the
largest
commercial
legal
search
service
in
terms
of
the
number
of
paying
subscribers
with
over
half
a
million
subscribers
performing
millions
of
searches
a
day
over
tens
of
terabytes
of
text
data
the
service
was
started
in
1975
in
2005
boolean
search
called
â€œterms
and
connectorsâ€
by
westlaw
was
still
the
default
and
used
by
a
large
percentage
of
users
although
ranked
free
text
querying
called
â€œnatural
languageâ€
by
westlaw
was
added
in
1992
here
are
some
example
boolean
queries
on
westlaw
information
need
information
on
the
legal
theories
involved
in
preventing
the
disclosure
of
trade
secrets
by
employees
formerly
employed
by
a
competing
company
query
trade
secret
/s
disclos
/s
prevent
/s
employe
information
need
requirements
for
disabled
people
to
be
able
to
access
a
workplace
query
disab
/p
access
/s
work-site
work-place
employment
/3
place
information
need
cases
about
a
hostâ€™s
responsibility
for
drunk
guests
query
host
/p
responsib
liab
/p
intoxicat
drunk
/p
guest
note
the
long
precise
queries
and
the
use
of
proximity
operators
both
uncommon
in
web
search
submitted
queries
average
about
ten
words
in
length
unlike
web
search
conventions
a
space
between
words
represents
disjunction
the
tightest
binding
operator
&
is
and
and
/s
/p
and
/k
ask
for
matches
in
the
same
sentence
same
paragraph
or
within
k
words
respectively
double
quotes
give
a
phrase
search
consecutive
words
see
section
2.4
page
39
the
exclamation
mark

gives
a
trailing
wildcard
query
see
section
3.2
page
51
thus
liab
matches
all
words
starting
with
liab
additionally
work-site
matches
any
of
worksite
work-site
or
work
site
see
section
2.2.1
page
22
typical
expert
queries
are
usually
carefully
defined
and
incrementally
developed
until
they
obtain
what
look
to
be
good
results
to
the
user
many
users
particularly
professionals
prefer
boolean
query
models
boolean
queries
are
precise
a
document
either
matches
the
query
or
it
does
not
this
offers
the
user
greater
control
and
transparency
over
what
is
retrieved
and
some
domains
such
as
legal
materials
allow
an
effective
means
of
document
ranking
within
a
boolean
model
westlaw
returns
documents
in
reverse
chronological
order
which
is
in
practice
quite
effective
in
2007
the
majority
of
law
librarians
still
seem
to
recommend
terms
and
connectors
for
high
recall
searches
and
the
majority
of
legal
users
think
they
are
getting
greater
control
by
using
them
however
this
does
not
mean
that
boolean
queries
are
more
effective
for
professional
searchers
indeed
experimenting
on
a
westlaw
subcollection
turtle
1994
found
that
free
text
queries
produced
better
results
than
boolean
queries
prepared
by
westlawâ€™s
own
reference
librarians
for
the
majority
of
the
information
needs
in
his
experiments
a
general
problem
with
boolean
search
is
that
using
and
operators
tends
to
produce
high
precision
but
low
recall
searches
while
using
or
operators
gives
low
precision
but
high
recall
searches
and
it
is
difficult
or
impossible
to
find
a
satisfactory
middle
ground
in
this
chapter
we
have
looked
at
the
structure
and
construction
of
a
basic
online
edition
c
2009
cambridge
up
16
1
boolean
retrieval
inverted
index
comprising
a
dictionary
and
postings
lists
we
introduced
the
boolean
retrieval
model
and
examined
how
to
do
efficient
retrieval
via
linear
time
merges
and
simple
query
optimization
in
chapters
2â€“7
we
will
consider
in
detail
richer
query
models
and
the
sort
of
augmented
index
structures
that
are
needed
to
handle
them
efficiently
here
we
just
mention
a
few
of
the
main
additional
things
we
would
like
to
be
able
to
do
1
we
would
like
to
better
determine
the
set
of
terms
in
the
dictionary
and
to
provide
retrieval
that
is
tolerant
to
spelling
mistakes
and
inconsistent
choice
of
words
2
it
is
often
useful
to
search
for
compounds
or
phrases
that
denote
a
concept
such
as
â€œoperating
systemâ€
as
the
westlaw
examples
show
we
might
also
wish
to
do
proximity
queries
such
as
gates
near
microsoft
to
answer
such
queries
the
index
has
to
be
augmented
to
capture
the
proximities
of
terms
in
documents
3
a
boolean
model
only
records
term
presence
or
absence
but
often
we
would
like
to
accumulate
evidence
giving
more
weight
to
documents
that
have
a
term
several
times
as
opposed
to
ones
that
contain
it
only
once
to
term
frequency
be
able
to
do
this
we
need
term
frequency
information
the
number
of
times
a
term
occurs
in
a
document
in
postings
lists
4
boolean
queries
just
retrieve
a
set
of
matching
documents
but
commonly
we
wish
to
have
an
effective
method
to
order
or
â€œrankâ€
the
returned
results
this
requires
having
a
mechanism
for
determining
a
document
score
which
encapsulates
how
good
a
match
a
document
is
for
a
query
with
these
additional
ideas
we
will
have
seen
most
of
the
basic
technology
that
supports
ad
hoc
searching
over
unstructured
information
ad
hoc
searching
over
documents
has
recently
conquered
the
world
powering
not
only
web
search
engines
but
the
kind
of
unstructured
search
that
lies
behind
the
large
ecommerce
websites
although
the
main
web
search
engines
differ
by
emphasizing
free
text
querying
most
of
the
basic
issues
and
technologies
of
indexing
and
querying
remain
the
same
as
we
will
see
in
later
chapters
moreover
over
time
web
search
engines
have
added
at
least
partial
implementations
of
some
of
the
most
popular
operators
from
extended
boolean
models
phrase
search
is
especially
popular
and
most
have
a
very
partial
implementation
of
boolean
operators
nevertheless
while
these
options
are
liked
by
expert
searchers
they
are
little
used
by
most
people
and
are
not
the
main
focus
in
work
on
trying
to
improve
web
search
engine
performance
?
exercise
1.12
â‹†
write
a
query
using
westlaw
syntax
which
would
find
any
of
the
words
professor
teacher
or
lecturer
in
the
same
sentence
as
a
form
of
the
verb
explain
online
edition
c
2009
cambridge
up
1.5
references
and
further
reading
17
exercise
1.13
â‹†
try
using
the
boolean
search
features
on
a
couple
of
major
web
search
engines
for
instance
choose
a
word
such
as
burglar
and
submit
the
queries
i
burglar
ii
burglar
and
burglar
and
iii
burglar
or
burglar
look
at
the
estimated
number
of
results
and
top
hits
do
they
make
sense
in
terms
of
boolean
logic?
often
they
havenâ€™t
for
major
search
engines
can
you
make
sense
of
what
is
going
on?
what
about
if
you
try
different
words?
for
example
query
for
i
knight
ii
conquer
and
then
iii
knight
or
conquer
what
bound
should
the
number
of
results
from
the
first
two
queries
place
on
the
third
query?
is
this
bound
observed?
1.5
references
and
further
reading
the
practical
pursuit
of
computerized
information
retrieval
began
in
the
late
1940s
cleverdon
1991
liddy
2005
a
great
increase
in
the
production
of
scientific
literature
much
in
the
form
of
less
formal
technical
reports
rather
than
traditional
journal
articles
coupled
with
the
availability
of
computers
led
to
interest
in
automatic
document
retrieval
however
in
those
days
document
retrieval
was
always
based
on
author
title
and
keywords
full-text
search
came
much
later
the
article
of
bush
1945
provided
lasting
inspiration
for
the
new
field
â€œconsider
a
future
device
for
individual
use
which
is
a
sort
of
mechanized
private
file
and
library
it
needs
a
name
and
to
coin
one
at
random
â€˜memexâ€™
will
do
a
memex
is
a
device
in
which
an
individual
stores
all
his
books
records
and
communications
and
which
is
mechanized
so
that
it
may
be
consulted
with
exceeding
speed
and
flexibility
it
is
an
enlarged
intimate
supplement
to
his
memory.â€
the
term
information
retrieval
was
coined
by
calvin
mooers
in
1948/1950
mooers
1950
in
1958
much
newspaper
attention
was
paid
to
demonstrations
at
a
conference
see
taube
and
wooster
1958
of
ibm
â€œauto-indexingâ€
machines
based
primarily
on
the
work
of
h
p
luhn
commercial
interest
quickly
gravitated
towards
boolean
retrieval
systems
but
the
early
years
saw
a
heady
debate
over
various
disparate
technologies
for
retrieval
systems
for
example
mooers
1961
dissented
â€œit
is
a
common
fallacy
underwritten
at
this
date
by
the
investment
of
several
million
dollars
in
a
variety
of
retrieval
hardware
that
the
algebra
of
george
boole
1847
is
the
appropriate
formalism
for
retrieval
system
design
this
view
is
as
widely
and
uncritically
accepted
as
it
is
wrong.â€
the
observation
of
and
vs
or
giving
you
opposite
extremes
in
a
precision/
recall
tradeoff
but
not
the
middle
ground
comes
from
lee
and
fox
1988
online
edition
c
2009
cambridge
up
18
1
boolean
retrieval
the
book
witten
et
al
1999
is
the
standard
reference
for
an
in-depth
comparison
of
the
space
and
time
efficiency
of
the
inverted
index
versus
other
possible
data
structures
a
more
succinct
and
up-to-date
presentation
appears
in
zobel
and
moffat
2006
we
further
discuss
several
approaches
in
chapter
5
regular
expressions
friedl
2006
covers
the
practical
usage
of
regular
expressions
for
searching
the
underlying
computer
science
appears
in
hopcroft
et
al
2000
online
edition
c
2009
cambridge
up
draft
â©
april
1
2009
cambridge
university
press
feedback
welcome
19
2
the
term
vocabulary
and
postings
lists
recall
the
major
steps
in
inverted
index
construction
1
collect
the
documents
to
be
indexed
2
tokenize
the
text
3
do
linguistic
preprocessing
of
tokens
4
index
the
documents
that
each
term
occurs
in
in
this
chapter
we
first
briefly
mention
how
the
basic
unit
of
a
document
can
be
defined
and
how
the
character
sequence
that
it
comprises
is
determined
section
2.1
we
then
examine
in
detail
some
of
the
substantive
linguistic
issues
of
tokenization
and
linguistic
preprocessing
which
determine
the
vocabulary
of
terms
which
a
system
uses
section
2.2
tokenization
is
the
process
of
chopping
character
streams
into
tokens
while
linguistic
preprocessing
then
deals
with
building
equivalence
classes
of
tokens
which
are
the
set
of
terms
that
are
indexed
indexing
itself
is
covered
in
chapters
1
and
4
then
we
return
to
the
implementation
of
postings
lists
in
section
2.3
we
examine
an
extended
postings
list
data
structure
that
supports
faster
querying
while
section
2.4
covers
building
postings
data
structures
suitable
for
handling
phrase
and
proximity
queries
of
the
sort
that
commonly
appear
in
both
extended
boolean
models
and
on
the
web
2.1
document
delineation
and
character
sequence
decoding
2.1.1
obtaining
the
character
sequence
in
a
document
digital
documents
that
are
the
input
to
an
indexing
process
are
typically
bytes
in
a
file
or
on
a
web
server
the
first
step
of
processing
is
to
convert
this
byte
sequence
into
a
linear
sequence
of
characters
for
the
case
of
plain
english
text
in
ascii
encoding
this
is
trivial
but
often
things
get
much
more
online
edition
c
2009
cambridge
up
20
2
the
term
vocabulary
and
postings
lists
complex
the
sequence
of
characters
may
be
encoded
by
one
of
various
single
byte
or
multibyte
encoding
schemes
such
as
unicode
utf-8
or
various
national
or
vendor-specific
standards
we
need
to
determine
the
correct
encoding
this
can
be
regarded
as
a
machine
learning
classification
problem
as
discussed
in
chapter
13
1
but
is
often
handled
by
heuristic
methods
user
selection
or
by
using
provided
document
metadata
once
the
encoding
is
determined
we
decode
the
byte
sequence
to
a
character
sequence
we
might
save
the
choice
of
encoding
because
it
gives
some
evidence
about
what
language
the
document
is
written
in
the
characters
may
have
to
be
decoded
out
of
some
binary
representation
like
microsoft
word
doc
files
and/or
a
compressed
format
such
as
zip
files
again
we
must
determine
the
document
format
and
then
an
appropriate
decoder
has
to
be
used
even
for
plain
text
documents
additional
decoding
may
need
to
be
done
in
xml
documents
section
10.1
page
197
character
entities
such
as
&amp
need
to
be
decoded
to
give
the
correct
character
namely
&
for
&amp
finally
the
textual
part
of
the
document
may
need
to
be
extracted
out
of
other
material
that
will
not
be
processed
this
might
be
the
desired
handling
for
xml
files
if
the
markup
is
going
to
be
ignored
we
would
almost
certainly
want
to
do
this
with
postscript
or
pdf
files
we
will
not
deal
further
with
these
issues
in
this
book
and
will
assume
henceforth
that
our
documents
are
a
list
of
characters
commercial
products
usually
need
to
support
a
broad
range
of
document
types
and
encodings
since
users
want
things
to
just
work
with
their
data
as
is
often
they
just
think
of
documents
as
text
inside
applications
and
are
not
even
aware
of
how
it
is
encoded
on
disk
this
problem
is
usually
solved
by
licensing
a
software
library
that
handles
decoding
document
formats
and
character
encodings
the
idea
that
text
is
a
linear
sequence
of
characters
is
also
called
into
question
by
some
writing
systems
such
as
arabic
where
text
takes
on
some
two
dimensional
and
mixed
order
characteristics
as
shown
in
figures
2.1
and
2.2
but
despite
some
complicated
writing
system
conventions
there
is
an
underlying
sequence
of
sounds
being
represented
and
hence
an
essentially
linear
structure
remains
and
this
is
what
is
represented
in
the
digital
representation
of
arabic
as
shown
in
figure
2.1
2.1.2
choosing
a
document
unit
document
unit
the
next
phase
is
to
determine
what
the
document
unit
for
indexing
is
thus
far
we
have
assumed
that
documents
are
fixed
units
for
the
purposes
of
indexing
for
example
we
take
each
file
in
a
folder
as
a
document
but
there
1
a
classifier
is
a
function
that
takes
objects
of
some
sort
and
assigns
them
to
one
of
a
number
of
distinct
classes
see
chapter
13
usually
classification
is
done
by
machine
learning
methods
such
as
probabilistic
models
but
it
can
also
be
done
by
hand-written
rules
online
edition
c
2009
cambridge
up
2.1
document
delineation
and
character
sequence
decoding
21
ùƒ
ù
øª
ø§
ø¨
ùœ
â‡
ø¢ùùžø¨ùœ
un
b
ä
t
i
k
/kitäbun/
â€˜a
bookâ€™
â—®
figure
2.1
an
example
of
a
vocalized
modern
standard
arabic
word
the
writing
is
from
right
to
left
and
letters
undergo
complex
mutations
as
they
are
combined
the
representation
of
short
vowels
here
/i/
and
/u/
and
the
final
/n/
nunation
departs
from
strict
linearity
by
being
represented
as
diacritics
above
and
below
letters
nevertheless
the
represented
text
is
still
clearly
a
linear
ordering
of
characters
representing
sounds
full
vocalization
as
here
normally
appears
only
in
the
koran
and
childrenâ€™s
books
day-to-day
text
is
unvocalized
short
vowels
are
not
represented
but
the
letter
for
a
would
still
appear
or
partially
vocalized
with
short
vow
â¯
els
inserted
in
places
where
the
writer
perceives
ambiguities
these
choices
add
further
complexities
to
indexing
ø§
ø§
ø§


1962

132

!"#
ø§

ù„
ø§



â†
â†’
â†
â†’
â†
start
â€˜algeria
achieved
its
independence
in
1962
after
132
years
of
french
occupation.â€™
â—®
figure
2.2
the
conceptual
linear
order
of
characters
is
not
necessarily
the
order
that
you
see
on
the
page
in
languages
that
are
written
right-to-left
such
as
hebrew
and
arabic
it
is
quite
common
to
also
have
left-to-right
text
interspersed
such
as
numbers
and
dollar
amounts
with
modern
unicode
representation
concepts
the
order
of
characters
in
files
matches
the
conceptual
order
and
the
reversal
of
displayed
characters
is
handled
by
the
rendering
system
but
this
may
not
be
true
for
documents
in
older
encodings
are
many
cases
in
which
you
might
want
to
do
something
different
a
traditional
unix
mbox-format
email
file
stores
a
sequence
of
email
messages
an
email
folder
in
one
file
but
you
might
wish
to
regard
each
email
message
as
a
separate
document
many
email
messages
now
contain
attached
documents
and
you
might
then
want
to
regard
the
email
message
and
each
contained
attachment
as
separate
documents
if
an
email
message
has
an
attached
zip
file
you
might
want
to
decode
the
zip
file
and
regard
each
file
it
contains
as
a
separate
document
going
in
the
opposite
direction
various
pieces
of
web
software
such
as
latex2html
take
things
that
you
might
regard
as
a
single
document
e.g
a
powerpoint
file
or
a
latex
document
and
split
them
into
separate
html
pages
for
each
slide
or
subsection
stored
as
separate
files
in
these
cases
you
might
want
to
combine
multiple
files
into
a
single
document
indexing
more
generally
for
very
long
documents
the
issue
of
indexing
granularity
granularity
arises
for
a
collection
of
books
it
would
usually
be
a
bad
idea
to
index
an
